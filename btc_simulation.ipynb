{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTC Fake - Training Completion Simulator\n",
    "\n",
    "This notebook gives the user a set of files that look just like files sent by BTC vendor to convey\n",
    "the Sephora Course Catalog, and the transactions of managers assigning training, and employees \n",
    "completing training. Its simulates Managers who assign Daily Dose trainings to employees, and\n",
    "simulates employees completing those manager-assigned trainings, PLUS training that was made by\n",
    "AI recommendations.\n",
    "\n",
    "## How it works:\n",
    "1. **Preprocessing**: Downloads two files that BTC sent to the SFTP server. These two files are \n",
    "the files that represent the training Course Catalog. These are meta-data - non transactional.\n",
    "2. **Manager Assigns Training**: \n",
    "   - Queries content_assignments AND content_completion tables from Databricks\n",
    "   - Calculates open assignments (assignments - completions) for each employee\n",
    "   - Selects and assigns up to 3 Daily Dose contents to all employees\n",
    "3. **Employee Completes Training**: \n",
    "   - Loads manager assignments and AI recommendations for each employee\n",
    "   - Completes training based on employee type (A, B, or F)\n",
    "4. **Output Generation**:\n",
    "   - NonCompletedAssignments CSV file (open assignments from Databricks + new manager assignments)\n",
    "   - ContentUserCompletion CSV file (completed training with source tracking)\n",
    "   - UserCompletion CSV file that is a dummy file required but not important\n",
    "5. **Update NonCompletedAssignments**: Removes completed assignments from the file\n",
    "6. **Summary**: Prints completion details showing which training came from manager vs AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import string\n",
    "from typing import List, Dict\n",
    "import urllib3\n",
    "import pytz\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Disable SSL warnings when ignoring certificate verification\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Define Pacific timezone globally for all timestamp calculations\n",
    "# Timestamps are calculated in PT for proper local time logic,\n",
    "# then converted to UTC (offset +00:00) for CSV output files\n",
    "PT = pytz.timezone('America/Los_Angeles')\n",
    "UTC = pytz.UTC\n",
    "\n",
    "# ML Training Recommender API Configuration\n",
    "API_BASE_URL = os.getenv(\"API_BASE_URL\", \"https://dataiku-api-devqa.lower.internal.sephora.com\")\n",
    "API_ENDPOINT = os.getenv(\"API_ENDPOINT\", \"/public/api/v1/mltr/v3/run\")\n",
    "API_TIMEOUT = int(os.getenv(\"API_TIMEOUT\", \"30\"))\n",
    "\n",
    "# File Path Configuration\n",
    "EMPLOYEES_FILE = os.getenv(\"EMPLOYEES_FILE\", \"input/employees.csv\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"generated_files\")\n",
    "SFTP_LOCAL_DIR = os.getenv(\"SFTP_LOCAL_DIR\", \"downloaded_files\")\n",
    "USER_COMPLETION_TEMPLATE_FILE = os.getenv(\"USER_COMPLETION_TEMPLATE_FILE\", \"docs/sample_files/UserCompletion_v2_YYYY_m_d_1_000001.csv\")\n",
    "\n",
    "# Databricks Configuration\n",
    "DATABRICKS_HOST = os.getenv(\"DATABRICKS_HOST\", \"\")\n",
    "DATABRICKS_HTTP_PATH = os.getenv(\"DATABRICKS_HTTP_PATH\", \"\")\n",
    "DATABRICKS_TOKEN = os.getenv(\"DATABRICKS_TOKEN\", \"\")\n",
    "DATABRICKS_CATALOG = os.getenv(\"DATABRICKS_CATALOG\", \"retail_systems_dev\")\n",
    "DATABRICKS_SCHEMA = os.getenv(\"DATABRICKS_SCHEMA\", \"store_enablement\")\n",
    "\n",
    "# SFTP Outbound Server Configuration (Publishing)\n",
    "SFTP_OUTBOUND_HOST = os.getenv(\"SFTP_OUTBOUND_HOST\", \"internal-sftp.sephoraus.com\")\n",
    "SFTP_OUTBOUND_USER = os.getenv(\"SFTP_OUTBOUND_USER\", \"SephoraRDIInternal\")\n",
    "SFTP_OUTBOUND_PASSWORD = os.getenv(\"SFTP_OUTBOUND_PASSWORD\", \"\")\n",
    "SFTP_OUTBOUND_REMOTE_PATH = os.getenv(\"SFTP_OUTBOUND_REMOTE_PATH\", \"/inbound/BTC/retailData/prod/vendor/mySephoraLearningV2\")\n",
    "SFTP_PUBLISH_ENABLED = os.getenv(\"SFTP_PUBLISH_ENABLED\", \"true\").lower() in ['true', '1', 'yes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - Download Files from SFTP\n",
    "\n",
    "This section prepares for a fresh run:\n",
    "\n",
    "## Cleanup\n",
    "1. Removes all files from `downloaded_files/` directory\n",
    "2. Removes all files from `generated_files/` directory\n",
    "3. Ensures each run starts with a clean slate\n",
    "\n",
    "## Generate UserCompletion File\n",
    "1. Copies the UserCompletion template from `docs/sample_files/`\n",
    "2. Renames it with the current date (YYYY_m_d format)\n",
    "3. Places it in `generated_files/` directory\n",
    "\n",
    "## Download Files from SFTP Server\n",
    "1. **CourseCatalog** - Training curriculum elements like Courses and components\n",
    "2. **StandAloneContent** - All training content (videos, PDFs, documents)\n",
    "\n",
    "These are the files sent recently by BTC to Prod and they are helpful for this\n",
    "process to know about training, like which Contents are Daily Dose.\n",
    "\n",
    "## Requirements:\n",
    "1. Copy `.env.example` to `.env` and add your SFTP password\n",
    "2. Files will be downloaded to `downloaded_files/` directory\n",
    "3. The system finds the most recent file based on the date in the filename\n",
    "\n",
    "## File Formats:\n",
    "- CourseCatalog: `CourseCatalog_V2_YYYY_M_DD_1_random.csv`\n",
    "- StandAloneContent: `StandAloneContent_v2_YYYY_M_DD_1_random.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Remove old files from previous runs\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def cleanup_directory(directory: str) -> int:\n",
    "    \"\"\"\n",
    "    Remove all files in a directory (keeps the directory itself and .gitkeep files).\n",
    "    \n",
    "    Args:\n",
    "        directory: Path to directory to clean\n",
    "    \n",
    "    Returns:\n",
    "        Number of files removed\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"  Directory does not exist: {directory}\")\n",
    "        return 0\n",
    "    \n",
    "    files_removed = 0\n",
    "    pattern = os.path.join(directory, \"*\")\n",
    "    \n",
    "    for file_path in glob.glob(pattern):\n",
    "        # Skip .gitkeep files\n",
    "        if os.path.basename(file_path) == \".gitkeep\":\n",
    "            continue\n",
    "        \n",
    "        # Only remove files, not subdirectories\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                files_removed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error removing {file_path}: {e}\")\n",
    "    \n",
    "    return files_removed\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING - Cleanup\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Cleaning up directories from previous runs...\")\n",
    "print()\n",
    "\n",
    "# Clean generated_files directory\n",
    "print(f\"Cleaning {OUTPUT_DIR}/...\")\n",
    "removed = cleanup_directory(OUTPUT_DIR)\n",
    "print(f\"  Removed {removed} file(s)\")\n",
    "print()\n",
    "\n",
    "# Clean downloaded_files directory\n",
    "print(f\"Cleaning {SFTP_LOCAL_DIR}/...\")\n",
    "removed = cleanup_directory(SFTP_LOCAL_DIR)\n",
    "print(f\"  Removed {removed} file(s)\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate UserCompletion file from template\n",
    "import shutil\n",
    "\n",
    "def generate_user_completion_file() -> str:\n",
    "    \"\"\"\n",
    "    Copy the UserCompletion template file to generated_files with current date and time in PT.\n",
    "    \n",
    "    Returns:\n",
    "        Path to the generated file, or None if generation fails\n",
    "    \"\"\"\n",
    "    # Source template file (now configurable via environment variable)\n",
    "    source_file = USER_COMPLETION_TEMPLATE_FILE\n",
    "    \n",
    "    if not os.path.exists(source_file):\n",
    "        print(f\"  Template file not found: {source_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Generate new filename with current date and time in PT\n",
    "    now = datetime.now(PT)\n",
    "    year = now.strftime(\"%Y\")\n",
    "    month = now.strftime(\"%-m\")  # No leading zero\n",
    "    day = now.strftime(\"%-d\")    # No leading zero\n",
    "    \n",
    "    # Generate 6-digit time suffix: HHMMSS\n",
    "    time_suffix = now.strftime(\"%H%M%S\")\n",
    "    \n",
    "    new_filename = f\"UserCompletion_v2_{year}_{month}_{day}_1_{time_suffix}.csv\"\n",
    "    destination_file = os.path.join(OUTPUT_DIR, new_filename)\n",
    "    \n",
    "    # Copy the file\n",
    "    try:\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "        return destination_file\n",
    "    except Exception as e:\n",
    "        print(f\"  Error copying file: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING - Generate UserCompletion File\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Generating a dummy UserCompletion file from sample file...\")\n",
    "user_completion_path = generate_user_completion_file()\n",
    "\n",
    "if user_completion_path:\n",
    "    print(f\"\u2713 UserCompletion file generated successfully\")\n",
    "    print(f\"  File: {user_completion_path}\")\n",
    "else:\n",
    "    print(\"\u2717 Failed to generate UserCompletion file\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SFTP libraries and load environment\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import paramiko\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# SFTP Inbound Server Configuration\n",
    "SFTP_INBOUND_HOST = os.getenv(\"SFTP_INBOUND_HOST\", \"sftp.sephora.com\")\n",
    "SFTP_INBOUND_USER = os.getenv(\"SFTP_INBOUND_USER\", \"SephoraMSL\")\n",
    "SFTP_INBOUND_PASSWORD = os.getenv(\"SFTP_INBOUND_PASSWORD\", \"your_SFTP_INBOUND_PASSWORD_placeholder\")\n",
    "SFTP_INBOUND_REMOTE_PATH = os.getenv(\"SFTP_INBOUND_REMOTE_PATH\", \"/inbound/BTC/retailData/prod/vendor/mySephoraLearning-archive\")\n",
    "\n",
    "def parse_course_catalog_filename(filename: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Parse course catalog filename to extract date components.\n",
    "    Format: CourseCatalog_V2_YYYY_M_DD_1_random.csv\n",
    "    \n",
    "    Args:\n",
    "        filename: The course catalog filename\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (year, month, day, datetime_obj) or None if parsing fails\n",
    "    \"\"\"\n",
    "    pattern = r'CourseCatalog_V2_(\\d{4})_(\\d{1,2})_(\\d{1,2})_\\d+_[a-z0-9]+\\.csv'\n",
    "    match = re.match(pattern, filename, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        year = int(match.group(1))\n",
    "        month = int(match.group(2))\n",
    "        day = int(match.group(3))\n",
    "        \n",
    "        try:\n",
    "            date_obj = datetime(year, month, day)\n",
    "            return (year, month, day, date_obj)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def parse_standalone_content_filename(filename: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Parse standalone content filename to extract date components.\n",
    "    Format: StandAloneContent_v2_YYYY_M_DD_1_random.csv\n",
    "    \n",
    "    Args:\n",
    "        filename: The standalone content filename\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (year, month, day, datetime_obj) or None if parsing fails\n",
    "    \"\"\"\n",
    "    pattern = r'StandAloneContent_v2_(\\d{4})_(\\d{1,2})_(\\d{1,2})_\\d+_[a-z0-9]+\\.csv'\n",
    "    match = re.match(pattern, filename, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        year = int(match.group(1))\n",
    "        month = int(match.group(2))\n",
    "        day = int(match.group(3))\n",
    "        \n",
    "        try:\n",
    "            date_obj = datetime(year, month, day)\n",
    "            return (year, month, day, date_obj)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def download_most_recent_course_catalog() -> str:\n",
    "    \"\"\"\n",
    "    Connect to SFTP inbound server and download the most recent CourseCatalog file.\n",
    "    \n",
    "    Returns:\n",
    "        Path to the downloaded file, or None if download fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create SFTP connection\n",
    "        transport = paramiko.Transport((SFTP_INBOUND_HOST, 22))\n",
    "        transport.connect(username=SFTP_INBOUND_USER, password=SFTP_INBOUND_PASSWORD)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "        \n",
    "        print(f\"Connected to SFTP inbound server: {SFTP_INBOUND_HOST}\")\n",
    "        \n",
    "        # Change to remote directory\n",
    "        sftp.chdir(SFTP_INBOUND_REMOTE_PATH)\n",
    "        print(f\"Changed to directory: {SFTP_INBOUND_REMOTE_PATH}\")\n",
    "        \n",
    "        # List all files in the directory\n",
    "        files = sftp.listdir()\n",
    "        print(f\"Found {len(files)} files in directory\")\n",
    "        \n",
    "        # Filter for course catalog files and parse dates\n",
    "        catalog_files = []\n",
    "        for filename in files:\n",
    "            parsed = parse_course_catalog_filename(filename)\n",
    "            if parsed:\n",
    "                catalog_files.append((filename, parsed[3]))  # (filename, datetime_obj)\n",
    "        \n",
    "        if not catalog_files:\n",
    "            print(\"No valid CourseCatalog files found\")\n",
    "            sftp.close()\n",
    "            transport.close()\n",
    "            return None\n",
    "        \n",
    "        # Sort by date (most recent first)\n",
    "        catalog_files.sort(key=lambda x: x[1], reverse=True)\n",
    "        most_recent_file = catalog_files[0][0]\n",
    "        most_recent_date = catalog_files[0][1]\n",
    "        \n",
    "        print(f\"Most recent file: {most_recent_file} (date: {most_recent_date.strftime('%Y-%m-%d')})\")\n",
    "        \n",
    "        # Download the file\n",
    "        local_path = os.path.join(SFTP_LOCAL_DIR, most_recent_file)\n",
    "        sftp.get(most_recent_file, local_path)\n",
    "        print(f\"Downloaded to: {local_path}\")\n",
    "        \n",
    "        # Close connections\n",
    "        sftp.close()\n",
    "        transport.close()\n",
    "        \n",
    "        return local_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading course catalog: {e}\")\n",
    "        print(f\"  SFTP Inbound Host: {SFTP_INBOUND_HOST}\")\n",
    "        print(f\"  SFTP Inbound Path: {SFTP_INBOUND_REMOTE_PATH}\")\n",
    "        print(f\"  SFTP Inbound User: {SFTP_INBOUND_USER}\")\n",
    "        return None\n",
    "\n",
    "def download_most_recent_standalone_content() -> str:\n",
    "    \"\"\"\n",
    "    Connect to SFTP inbound server and download the most recent StandAloneContent file.\n",
    "    \n",
    "    Returns:\n",
    "        Path to the downloaded file, or None if download fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create SFTP connection\n",
    "        transport = paramiko.Transport((SFTP_INBOUND_HOST, 22))\n",
    "        transport.connect(username=SFTP_INBOUND_USER, password=SFTP_INBOUND_PASSWORD)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "        \n",
    "        print(f\"Connected to SFTP inbound server: {SFTP_INBOUND_HOST}\")\n",
    "        \n",
    "        # Change to remote directory\n",
    "        sftp.chdir(SFTP_INBOUND_REMOTE_PATH)\n",
    "        print(f\"Changed to directory: {SFTP_INBOUND_REMOTE_PATH}\")\n",
    "        \n",
    "        # List all files in the directory\n",
    "        files = sftp.listdir()\n",
    "        print(f\"Found {len(files)} files in directory\")\n",
    "        \n",
    "        # Filter for standalone content files and parse dates\n",
    "        content_files = []\n",
    "        for filename in files:\n",
    "            parsed = parse_standalone_content_filename(filename)\n",
    "            if parsed:\n",
    "                content_files.append((filename, parsed[3]))  # (filename, datetime_obj)\n",
    "        \n",
    "        if not content_files:\n",
    "            print(\"No valid StandAloneContent files found\")\n",
    "            sftp.close()\n",
    "            transport.close()\n",
    "            return None\n",
    "        \n",
    "        # Sort by date (most recent first)\n",
    "        content_files.sort(key=lambda x: x[1], reverse=True)\n",
    "        most_recent_file = content_files[0][0]\n",
    "        most_recent_date = content_files[0][1]\n",
    "        \n",
    "        print(f\"Most recent file: {most_recent_file} (date: {most_recent_date.strftime('%Y-%m-%d')})\")\n",
    "        \n",
    "        # Download the file\n",
    "        local_path = os.path.join(SFTP_LOCAL_DIR, most_recent_file)\n",
    "        sftp.get(most_recent_file, local_path)\n",
    "        print(f\"Downloaded to: {local_path}\")\n",
    "        \n",
    "        # Close connections\n",
    "        sftp.close()\n",
    "        transport.close()\n",
    "        \n",
    "        return local_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading standalone content: {e}\")\n",
    "        print(f\"  SFTP Inbound Host: {SFTP_INBOUND_HOST}\")\n",
    "        print(f\"  SFTP Inbound Path: {SFTP_INBOUND_REMOTE_PATH}\")\n",
    "        print(f\"  SFTP Inbound User: {SFTP_INBOUND_USER}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_files_to_sftp_outbound(files_to_publish: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Publish generated files to SFTP outbound server.\n",
    "    \n",
    "    Args:\n",
    "        files_to_publish: List of local file paths to upload\n",
    "    \n",
    "    Returns:\n",
    "        True if all files published successfully, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if publishing is enabled\n",
    "    if not SFTP_PUBLISH_ENABLED:\n",
    "        print(\"SFTP publishing is disabled (SFTP_PUBLISH_ENABLED=false)\")\n",
    "        return False\n",
    "    \n",
    "    # Check if password is configured\n",
    "    if not SFTP_OUTBOUND_PASSWORD:\n",
    "        print(\"ERROR: SFTP outbound password not configured\")\n",
    "        print(\"  Set SFTP_OUTBOUND_PASSWORD in .env file\")\n",
    "        return False\n",
    "    \n",
    "    # Check if we have files to publish\n",
    "    if not files_to_publish:\n",
    "        print(\"No files to publish\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create SFTP connection\n",
    "        print(f\"Connecting to SFTP outbound server: {SFTP_OUTBOUND_HOST}\")\n",
    "        transport = paramiko.Transport((SFTP_OUTBOUND_HOST, 22))\n",
    "        transport.connect(username=SFTP_OUTBOUND_USER, password=SFTP_OUTBOUND_PASSWORD)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "        \n",
    "        print(f\"\u2713 Connected successfully\")\n",
    "        \n",
    "        # Change to remote directory\n",
    "        try:\n",
    "            sftp.chdir(SFTP_OUTBOUND_REMOTE_PATH)\n",
    "            print(f\"\u2713 Changed to directory: {SFTP_OUTBOUND_REMOTE_PATH}\")\n",
    "        except IOError:\n",
    "            print(f\"ERROR: Remote directory does not exist: {SFTP_OUTBOUND_REMOTE_PATH}\")\n",
    "            sftp.close()\n",
    "            transport.close()\n",
    "            return False\n",
    "        \n",
    "        # Upload each file\n",
    "        published_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        for local_file_path in files_to_publish:\n",
    "            if not os.path.exists(local_file_path):\n",
    "                print(f\"  \u26a0 File not found (skipping): {local_file_path}\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Get just the filename (not the full path)\n",
    "            filename = os.path.basename(local_file_path)\n",
    "            \n",
    "            try:\n",
    "                # Upload the file\n",
    "                sftp.put(local_file_path, filename)\n",
    "                print(f\"  \u2713 Uploaded: {filename}\")\n",
    "                published_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  \u2717 Failed to upload {filename}: {e}\")\n",
    "                failed_count += 1\n",
    "        \n",
    "        # Close connections\n",
    "        sftp.close()\n",
    "        transport.close()\n",
    "        \n",
    "        # Summary\n",
    "        print()\n",
    "        print(f\"Publishing summary:\")\n",
    "        print(f\"  Successfully published: {published_count} file(s)\")\n",
    "        if failed_count > 0:\n",
    "            print(f\"  Failed: {failed_count} file(s)\")\n",
    "        \n",
    "        return failed_count == 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to publish files to SFTP outbound server\")\n",
    "        print(f\"  Error: {e}\")\n",
    "        print(f\"  Host: {SFTP_OUTBOUND_HOST}\")\n",
    "        print(f\"  User: {SFTP_OUTBOUND_USER}\")\n",
    "        print(f\"  Remote Path: {SFTP_OUTBOUND_REMOTE_PATH}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute: Download Course Catalog and Standalone Content from SFTP\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING - Download Files from SFTP Inbound Server\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Download Course Catalog\n",
    "print(\"Downloading Course Catalog...\")\n",
    "print(\"-\" * 80)\n",
    "course_catalog_path = download_most_recent_course_catalog()\n",
    "\n",
    "if course_catalog_path:\n",
    "    print()\n",
    "    print(f\"\u2713 Course catalog downloaded successfully\")\n",
    "    print(f\"  File: {course_catalog_path}\")\n",
    "    \n",
    "    # Optionally load and preview the file\n",
    "    try:\n",
    "        catalog_df = pd.read_csv(course_catalog_path)\n",
    "        print(f\"  Rows: {len(catalog_df)}\")\n",
    "        print(f\"  Columns: {list(catalog_df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Note: Could not preview file: {e}\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"\u2717 Failed to download course catalog\")\n",
    "    print(\"  Please check:\")\n",
    "    print(\"    1. .env file contains valid SFTP_INBOUND_PASSWORD\")\n",
    "    print(\"    2. SFTP inbound server is accessible\")\n",
    "    print(\"    3. Remote path exists and contains CourseCatalog files\")\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Download Standalone Content\n",
    "print(\"Downloading Standalone Content...\")\n",
    "print(\"-\" * 80)\n",
    "standalone_content_path = download_most_recent_standalone_content()\n",
    "\n",
    "if standalone_content_path:\n",
    "    print()\n",
    "    print(f\"\u2713 Standalone content downloaded successfully\")\n",
    "    print(f\"  File: {standalone_content_path}\")\n",
    "    \n",
    "    # Optionally load and preview the file\n",
    "    try:\n",
    "        content_df = pd.read_csv(standalone_content_path)\n",
    "        print(f\"  Rows: {len(content_df)}\")\n",
    "        print(f\"  Columns: {list(content_df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Note: Could not preview file: {e}\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"\u2717 Failed to download standalone content\")\n",
    "    print(\"  Please check:\")\n",
    "    print(\"    1. .env file contains valid SFTP_INBOUND_PASSWORD\")\n",
    "    print(\"    2. SFTP inbound server is accessible\")\n",
    "    print(\"    3. Remote path exists and contains StandAloneContent files\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load employees (used by both Manager and Employee Training sections)\n",
    "print(f\"Loading employees from {EMPLOYEES_FILE}...\")\n",
    "employees_df = pd.read_csv(EMPLOYEES_FILE)\n",
    "\n",
    "# Filter out comment rows (rows where employee_id starts with '#')\n",
    "initial_count = len(employees_df)\n",
    "employees_df['employee_id'] = employees_df['employee_id'].astype(str)\n",
    "employees_df = employees_df[~employees_df['employee_id'].str.startswith('#')].copy()\n",
    "\n",
    "# Convert employee_id back to int after filtering comments\n",
    "employees_df['employee_id'] = employees_df['employee_id'].astype(int)\n",
    "\n",
    "filtered_count = initial_count - len(employees_df)\n",
    "\n",
    "if filtered_count > 0:\n",
    "    print(f\"Filtered out {filtered_count} comment row(s)\")\n",
    "\n",
    "print(f\"Loaded {len(employees_df)} employees\")\n",
    "print()\n",
    "\n",
    "# Helper function for formatting content IDs (used by both sections)\n",
    "def format_content_id(content_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Format content ID with commas for human readability.\n",
    "    Example: 1915085 -> \"1,915,085\"\n",
    "    \n",
    "    Args:\n",
    "        content_id: The numeric content ID\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with commas\n",
    "    \"\"\"\n",
    "    return f\"{content_id:,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manager - Assign Training to Employees\n",
    "\n",
    "This section implements the manager functionality:\n",
    "1. Queries Databricks for employee state using BOTH content_assignments and content_completion tables\n",
    "2. Calculates open assignments (assignments - completions) for each employee\n",
    "3. Loads the standalone content file from preprocessing\n",
    "4. Filters for content where Daily_Dose_BA is TRUE\n",
    "5. Sorts by CreateDate (most recent first)\n",
    "6. Selects up to 3 Daily Dose contents to assign\n",
    "7. Checks for Daily Dose conflicts (employees to skip):\n",
    "   - **Conflict Check 1**: Queries content_completion table for employees who completed ANY Daily Dose this week\n",
    "   - **Conflict Check 2**: Checks open assignments for Daily Dose with due dates this week\n",
    "8. Skips employees who have Daily Dose conflicts\n",
    "9. Assigns the 3 Daily Dose contents to all eligible employees\n",
    "10. Assigns 1 random non-Daily Dose content to ALL employees (no skipping)\n",
    "11. Generates a NonCompletedAssignments CSV file with:\n",
    "    - Open assignments from Databricks (written FIRST)\n",
    "    - New manager assignments (written SECOND)\n",
    "\n",
    "**Note**: The manager will NOT assign Daily Dose to an employee if they already completed ANY Daily Dose in the current week (Sunday to Sunday).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "# Date/time helper functions - all use PT timezone defined in Cell 2\n",
    "\n",
    "def get_sunday_of_current_week() -> datetime:\n",
    "    \"\"\"\n",
    "    Get Sunday of the current week at 23:59:59 PT.\n",
    "    \n",
    "    If today is Sunday, returns today at 23:59:59.\n",
    "    If today is Monday-Saturday, returns the most recent past Sunday at 23:59:59.\n",
    "    \n",
    "    Returns:\n",
    "        datetime object for Sunday of current week at 23:59:59 PT\n",
    "    \"\"\"\n",
    "    now = datetime.now(PT)\n",
    "    # Sunday is 6 in Python's weekday() (Monday=0, Sunday=6)\n",
    "    current_weekday = now.weekday()\n",
    "    \n",
    "    if current_weekday == 6:\n",
    "        # Today is Sunday\n",
    "        sunday = now\n",
    "    else:\n",
    "        # Go back to the most recent Sunday\n",
    "        # Monday=0, so we need to go back (current_weekday + 1) days to reach Sunday\n",
    "        days_since_sunday = current_weekday + 1\n",
    "        sunday = now - timedelta(days=days_since_sunday)\n",
    "    \n",
    "    # Set time to 23:59:59 PT\n",
    "    return sunday.replace(hour=23, minute=59, second=59, microsecond=0)\n",
    "\n",
    "def get_next_future_sunday() -> datetime:\n",
    "    \"\"\"\n",
    "    Get the next future Sunday at 22:49:49 PT.\n",
    "    \n",
    "    If today is Sunday, returns next Sunday (7 days from now).\n",
    "    If today is Monday-Saturday, returns the upcoming Sunday.\n",
    "    \n",
    "    Returns:\n",
    "        datetime object for the next future Sunday at 22:49:49 PT\n",
    "    \"\"\"\n",
    "    now = datetime.now(PT)\n",
    "    # Sunday is 6 in Python's weekday() (Monday=0, Sunday=6)\n",
    "    current_weekday = now.weekday()\n",
    "    \n",
    "    if current_weekday == 6:\n",
    "        # Today is Sunday, next Sunday is 7 days away\n",
    "        days_until_sunday = 7\n",
    "    else:\n",
    "        # Calculate days until next Sunday\n",
    "        # Monday=0, Tuesday=1, ..., Saturday=5\n",
    "        # Days to Sunday: 6 - current_weekday\n",
    "        days_until_sunday = 6 - current_weekday\n",
    "    \n",
    "    next_sunday = now + timedelta(days=days_until_sunday)\n",
    "    \n",
    "    # Set time to 22:49:49 PT\n",
    "    return next_sunday.replace(hour=22, minute=49, second=49, microsecond=0)\n",
    "\n",
    "def generate_request_id() -> str:\n",
    "    \"\"\"\n",
    "    Generate RequestId in format: fake:DD\n",
    "    Example: fake:14 (for the 14th day of the month)\n",
    "    Uses PT timezone for date component.\n",
    "    \n",
    "    Returns:\n",
    "        RequestId string\n",
    "    \"\"\"\n",
    "    now = datetime.now(PT)\n",
    "    day = now.strftime(\"%d\")  # 2-digit day with leading zero\n",
    "    \n",
    "    return f\"fake:{day}\"\n",
    "\n",
    "def generate_non_completed_assignments_filename() -> str:\n",
    "    \"\"\"\n",
    "    Generate NonCompletedAssignments filename with timestamp.\n",
    "    Format: Non_Completed_Assignments_V2_YYYY_M_DD_1_HHMMSS.csv\n",
    "    Uses PT timezone for date and time components.\n",
    "    \n",
    "    Returns:\n",
    "        Generated filename\n",
    "    \"\"\"\n",
    "    now = datetime.now(PT)\n",
    "    year = now.strftime(\"%Y\")\n",
    "    month = now.strftime(\"%-m\")  # No leading zero\n",
    "    day = now.strftime(\"%-d\")    # No leading zero\n",
    "    \n",
    "    # Generate 6-digit time suffix: HHMMSS\n",
    "    time_suffix = now.strftime(\"%H%M%S\")\n",
    "    \n",
    "    return f\"Non_Completed_Assignments_V2_{year}_{month}_{day}_1_{time_suffix}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_assignments_from_databricks(employee_ids: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query Databricks to get open (non-completed) assignments for specific employees.\n",
    "    \n",
    "    Open assignments = content_assignments - content_completion\n",
    "    \n",
    "    Args:\n",
    "        employee_ids: List of employee IDs (ba_id) to query assignments for\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: ba_id, content_id, assignment_date, assignment_begin_date, \n",
    "                               assignment_due_date, content_type\n",
    "        Returns empty DataFrame if Databricks is not configured (allows process to continue).\n",
    "        Raises exception if Databricks IS configured but query fails (stops process).\n",
    "    \"\"\"\n",
    "    # Check if Databricks is configured\n",
    "    if not all([DATABRICKS_HOST, DATABRICKS_HTTP_PATH, DATABRICKS_TOKEN]):\n",
    "        print(\"Databricks configuration not found. Skipping assignments query.\")\n",
    "        print(\"  Set DATABRICKS_HOST, DATABRICKS_HTTP_PATH, and DATABRICKS_TOKEN in .env file\")\n",
    "        print(\"  Process will continue with only new manager assignments.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Check if we have employee IDs to query\n",
    "    if not employee_ids:\n",
    "        print(\"No employee IDs provided. Skipping assignments query.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Databricks IS configured - any error should stop the process\n",
    "    try:\n",
    "        from databricks import sql\n",
    "    except ImportError as e:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ERROR: databricks-sql-connector not installed\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        print(\"The databricks-sql-connector library is required but not installed.\")\n",
    "        print()\n",
    "        print(\"To fix this issue:\")\n",
    "        print(\"  1. Activate your virtual environment:\")\n",
    "        print(\"     source .venv/bin/activate  # macOS/Linux\")\n",
    "        print()\n",
    "        print(\"  2. Install the required library:\")\n",
    "        print(\"     pip install databricks-sql-connector\")\n",
    "        print()\n",
    "        print(\"  OR install all requirements:\")\n",
    "        print(\"     pip install -r requirements.txt\")\n",
    "        print()\n",
    "        print(\"=\" * 80)\n",
    "        raise ImportError(\"databricks-sql-connector not installed\") from e\n",
    "    \n",
    "    try:\n",
    "        # Connect to Databricks\n",
    "        print(f\"Connecting to Databricks...\")\n",
    "        print(f\"  Host: {DATABRICKS_HOST}\")\n",
    "        print(f\"  HTTP Path: {DATABRICKS_HTTP_PATH}\")\n",
    "        \n",
    "        connection = sql.connect(\n",
    "            server_hostname=DATABRICKS_HOST,\n",
    "            http_path=DATABRICKS_HTTP_PATH,\n",
    "            access_token=DATABRICKS_TOKEN\n",
    "        )\n",
    "        \n",
    "        print(f\"\u2713 Connected successfully\")\n",
    "        \n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Table names\n",
    "        assignments_table = f\"{DATABRICKS_CATALOG}.{DATABRICKS_SCHEMA}.content_assignments\"\n",
    "        completion_table = f\"{DATABRICKS_CATALOG}.{DATABRICKS_SCHEMA}.content_completion\"\n",
    "        \n",
    "        print(f\"Querying tables:\")\n",
    "        print(f\"  Assignments: {assignments_table}\")\n",
    "        print(f\"  Completions: {completion_table}\")\n",
    "        print(f\"  For {len(employee_ids)} employee(s): {sorted(employee_ids)}\")\n",
    "        \n",
    "        # Build IN clause for employee IDs\n",
    "        employee_ids_str = \", \".join([str(emp_id) for emp_id in employee_ids])\n",
    "        \n",
    "        # Query: Get all assignments, then LEFT JOIN with completions to find open ones\n",
    "        # Open assignments are those where completion.ba_id IS NULL (no matching completion)\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            a.ba_id,\n",
    "            a.content_id,\n",
    "            a.assignment_date,\n",
    "            a.assignment_begin_date,\n",
    "            a.assignment_due_date,\n",
    "            a.content_type\n",
    "        FROM {assignments_table} a\n",
    "        LEFT JOIN {completion_table} c\n",
    "            ON a.ba_id = c.ba_id \n",
    "            AND a.content_id = c.content_id\n",
    "        WHERE a.ba_id IN ({employee_ids_str})\n",
    "            AND c.ba_id IS NULL\n",
    "        ORDER BY a.ba_id, a.assignment_due_date\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # Fetch results\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        \n",
    "        print(f\"\u2713 Retrieved {len(df)} open assignment(s) from Databricks\")\n",
    "        print(f\"  (Open = assignments NOT in completions)\")\n",
    "        if len(df) > 0:\n",
    "            unique_employees = df['ba_id'].nunique()\n",
    "            print(f\"  Open assignments for {unique_employees} employee(s)\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ERROR: Failed to query Databricks\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        print()\n",
    "        print(\"Connection details:\")\n",
    "        print(f\"  Host: {DATABRICKS_HOST}\")\n",
    "        print(f\"  HTTP Path: {DATABRICKS_HTTP_PATH}\")\n",
    "        print(f\"  Catalog: {DATABRICKS_CATALOG}\")\n",
    "        print(f\"  Schema: {DATABRICKS_SCHEMA}\")\n",
    "        print(f\"  Assignments table: {DATABRICKS_CATALOG}.{DATABRICKS_SCHEMA}.content_assignments\")\n",
    "        print(f\"  Completion table: {DATABRICKS_CATALOG}.{DATABRICKS_SCHEMA}.content_completion\")\n",
    "        print()\n",
    "        print(\"Common issues:\")\n",
    "        print(\"  1. Invalid or expired access token\")\n",
    "        print(\"     \u2192 Generate new token in Databricks: User Settings \u2192 Access Tokens\")\n",
    "        print()\n",
    "        print(\"  2. Incorrect hostname or HTTP path\")\n",
    "        print(\"     \u2192 Verify DATABRICKS_HOST and DATABRICKS_HTTP_PATH in .env\")\n",
    "        print()\n",
    "        print(\"  3. Table does not exist\")\n",
    "        print(\"     \u2192 Verify both tables exist:\")\n",
    "        print(f\"       {DATABRICKS_CATALOG}.{DATABRICKS_SCHEMA}.content_assignments\")\n",
    "        print(f\"       {DATABRICKS_CATALOG}.{DATABRICKS_SCHEMA}.content_completion\")\n",
    "        print()\n",
    "        print(\"  4. Network/firewall issues\")\n",
    "        print(\"     \u2192 Ensure you can reach the Databricks workspace\")\n",
    "        print()\n",
    "        print(\"  5. Insufficient permissions\")\n",
    "        print(\"     \u2192 Verify your account has SELECT permission on both tables\")\n",
    "        print()\n",
    "        print(\"=\" * 80)\n",
    "        raise RuntimeError(f\"Databricks query failed: {str(e)}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manager - Query Databricks and select training content to assign to employees\n",
    "print(\"=\" * 80)\n",
    "print(\"MANAGER - Assigning Training to Employees\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Step 1: Get list of employee IDs from input file and query Databricks\n",
    "employee_ids_list = employees_df['employee_id'].tolist()\n",
    "print(f\"Querying Databricks for assignments for {len(employee_ids_list)} employees from input file...\")\n",
    "print(\"-\" * 80)\n",
    "open_assignments_df = get_open_assignments_from_databricks(employee_ids_list)\n",
    "print()\n",
    "\n",
    "# Convert Databricks assignments to the NonCompletedAssignments format\n",
    "databricks_assignments = []\n",
    "if not open_assignments_df.empty:\n",
    "    for _, row in open_assignments_df.iterrows():\n",
    "        databricks_assignments.append({\n",
    "            \"UserID\": int(row['ba_id']),\n",
    "            \"CreateDate_text\": row['assignment_date'].isoformat() if hasattr(row['assignment_date'], 'isoformat') else str(row['assignment_date']),\n",
    "            \"RequestId\": generate_request_id(),\n",
    "            \"TrainingElementId\": format_content_id(int(row['content_id'])),\n",
    "            \"Start_Date_text\": row['assignment_begin_date'].isoformat() if hasattr(row['assignment_begin_date'], 'isoformat') else str(row['assignment_begin_date']),\n",
    "            \"DueDate_text\": row['assignment_due_date'].isoformat() if hasattr(row['assignment_due_date'], 'isoformat') else str(row['assignment_due_date']),\n",
    "            \"ContentType\": row['content_type'] if 'content_type' in row else \"Media\"\n",
    "        })\n",
    "    print(f\"Converted {len(databricks_assignments)} Databricks assignment(s) to output format\")\n",
    "    print()\n",
    "\n",
    "# Step 2: Load the standalone content file\n",
    "new_manager_assignments = []\n",
    "employee_assigned_daily_dose = {}  # Track Daily Dose assignments\n",
    "employee_assigned_random = {}  # Track random non-Daily Dose assignments\n",
    "\n",
    "if standalone_content_path and os.path.exists(standalone_content_path):\n",
    "    print(f\"Loading standalone content from: {standalone_content_path}\")\n",
    "    standalone_df = pd.read_csv(standalone_content_path)\n",
    "    print(f\"Loaded {len(standalone_df)} content items\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate dates for NEW assignments in PT, then convert to UTC for CSV output\n",
    "    created_date = datetime.now(PT).astimezone(UTC).isoformat()\n",
    "    start_date = get_sunday_of_current_week().astimezone(UTC).isoformat()\n",
    "    due_date = get_next_future_sunday().astimezone(UTC).isoformat()\n",
    "    \n",
    "    # PART A: DAILY DOSE ASSIGNMENTS\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PART A: DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Filter for content where Daily_Dose_BA is TRUE\n",
    "    print(\"Filtering for Daily Dose training (Daily_Dose_BA = TRUE)...\")\n",
    "    daily_dose_content = standalone_df[\n",
    "        (standalone_df['Daily_Dose_BA'] == 'TRUE') | \n",
    "        (standalone_df['Daily_Dose_BA'] == True)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Found {len(daily_dose_content)} Daily Dose content items\")\n",
    "    print()\n",
    "    \n",
    "    if len(daily_dose_content) > 0:\n",
    "        # Convert CreateDate to datetime for sorting\n",
    "        daily_dose_content['CreateDate_dt'] = pd.to_datetime(daily_dose_content['CreateDate'])\n",
    "        \n",
    "        # Sort by CreateDate (most recent first)\n",
    "        daily_dose_content = daily_dose_content.sort_values('CreateDate_dt', ascending=False)\n",
    "        \n",
    "        # Select up to 3 most recent contents\n",
    "        contents_to_assign = daily_dose_content.head(3)\n",
    "        \n",
    "        print(f\"Selected {len(contents_to_assign)} Daily Dose content(s) to assign:\")\n",
    "        for idx, content in contents_to_assign.iterrows():\n",
    "            content_id = content['ContentId']\n",
    "            content_name = content['ContentName']\n",
    "            create_date = content['CreateDate']\n",
    "            print(f\"  {format_content_id(int(content_id.replace(',', '')))} - {content_name} (Created: {create_date})\")\n",
    "        print()\n",
    "        \n",
    "        # Check which employees to skip for Daily Dose\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Checking for Daily Dose conflicts...\")\n",
    "        print()\n",
    "        \n",
    "        # Calculate current week boundaries (Sunday to Sunday)\n",
    "        now_pt = datetime.now(PT)\n",
    "        sunday_of_current_week = get_sunday_of_current_week()\n",
    "        next_sunday = get_next_future_sunday()\n",
    "        \n",
    "        # For completion date comparison, use date objects\n",
    "        week_start_date = sunday_of_current_week.date()\n",
    "        week_end_date = next_sunday.date()\n",
    "        \n",
    "        print(f\"Current week (for completion check): {week_start_date} to {week_end_date}\")\n",
    "        print()\n",
    "        \n",
    "        # Build set of Daily Dose content IDs for quick lookup\n",
    "        daily_dose_content_ids = set()\n",
    "        for _, dd_content in standalone_df[\n",
    "            (standalone_df['Daily_Dose_BA'] == 'TRUE') | \n",
    "            (standalone_df['Daily_Dose_BA'] == True)\n",
    "        ].iterrows():\n",
    "            content_id_str = dd_content['ContentId']\n",
    "            if isinstance(content_id_str, str):\n",
    "                content_id_numeric = int(content_id_str.replace(',', ''))\n",
    "            else:\n",
    "                content_id_numeric = int(content_id_str)\n",
    "            daily_dose_content_ids.add(content_id_numeric)\n",
    "        \n",
    "        # Check employees for Daily Dose conflicts\n",
    "        employees_to_skip_dd = {}  # Map employee_id -> reason for skipping Daily Dose\n",
    "        \n",
    "        # CONFLICT CHECK 1: Query content_completion for Daily Dose completions this week\n",
    "        if all([DATABRICKS_HOST, DATABRICKS_HTTP_PATH, DATABRICKS_TOKEN]):\n",
    "            try:\n",
    "                from databricks import sql\n",
    "                \n",
    "                # Connect to Databricks\n",
    "                connection = sql.connect(\n",
    "                    server_hostname=DATABRICKS_HOST,\n",
    "                    http_path=DATABRICKS_HTTP_PATH,\n",
    "                    access_token=DATABRICKS_TOKEN\n",
    "                )\n",
    "                \n",
    "                cursor = connection.cursor()\n",
    "                \n",
    "                # Table name\n",
    "                completion_table = f\"{DATABRICKS_CATALOG}.{DATABRICKS_SCHEMA}.content_completion\"\n",
    "                \n",
    "                print(f\"Querying {completion_table} for Daily Dose completions this week...\")\n",
    "                \n",
    "                # Build IN clause for employee IDs and Daily Dose content IDs\n",
    "                employee_ids_str = \", \".join([str(emp_id) for emp_id in employee_ids_list])\n",
    "                dd_content_ids_str = \", \".join([str(cid) for cid in daily_dose_content_ids])\n",
    "                \n",
    "                # Query: Find employees who completed ANY Daily Dose content this week\n",
    "                query = f\"\"\"\n",
    "                SELECT \n",
    "                    ba_id,\n",
    "                    content_id,\n",
    "                    completion_date\n",
    "                FROM {completion_table}\n",
    "                WHERE ba_id IN ({employee_ids_str})\n",
    "                    AND content_id IN ({dd_content_ids_str})\n",
    "                    AND completion_date >= '{week_start_date}'\n",
    "                    AND completion_date <= '{week_end_date}'\n",
    "                ORDER BY ba_id, completion_date DESC\n",
    "                \"\"\"\n",
    "                \n",
    "                cursor.execute(query)\n",
    "                \n",
    "                # Fetch results\n",
    "                completion_rows = cursor.fetchall()\n",
    "                \n",
    "                # Close connection\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "                \n",
    "                if completion_rows:\n",
    "                    print(f\"Found {len(completion_rows)} Daily Dose completion(s) this week:\")\n",
    "                    for row in completion_rows:\n",
    "                        emp_id = row[0]\n",
    "                        content_id = row[1]\n",
    "                        comp_date = row[2]\n",
    "                        \n",
    "                        # Mark this employee to skip\n",
    "                        if emp_id not in employees_to_skip_dd:\n",
    "                            employees_to_skip_dd[emp_id] = {\n",
    "                                'reason': 'completed_this_week',\n",
    "                                'content_id': content_id,\n",
    "                                'completion_date': comp_date\n",
    "                            }\n",
    "                        \n",
    "                        print(f\"  Employee {emp_id}: Completed Daily Dose {content_id} on {comp_date}\")\n",
    "                    print()\n",
    "                else:\n",
    "                    print(\"\u2713 No Daily Dose completions found this week\")\n",
    "                    print()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0 Could not query content_completion: {e}\")\n",
    "                print(\"  Continuing without completion check...\")\n",
    "                print()\n",
    "        \n",
    "        # CONFLICT CHECK 2: Check open assignments for Daily Dose with due dates this week\n",
    "        today_date = now_pt.date()\n",
    "        next_sunday_date = next_sunday.date()\n",
    "        \n",
    "        if not open_assignments_df.empty:\n",
    "            for _, assignment in open_assignments_df.iterrows():\n",
    "                employee_id = int(assignment['ba_id'])\n",
    "                content_id = int(assignment['content_id'])\n",
    "                assignment_due_date = assignment['assignment_due_date']\n",
    "                \n",
    "                # Check if this is a Daily Dose assignment\n",
    "                if content_id in daily_dose_content_ids:\n",
    "                    # Convert due date to date for comparison\n",
    "                    if hasattr(assignment_due_date, 'date'):\n",
    "                        due_date_check = assignment_due_date.date()\n",
    "                    else:\n",
    "                        from dateutil import parser\n",
    "                        due_date_check = parser.parse(str(assignment_due_date)).date()\n",
    "                    \n",
    "                    # Check if due date is today or next future Sunday\n",
    "                    if due_date_check == today_date or due_date_check == next_sunday_date:\n",
    "                        if employee_id not in employees_to_skip_dd:\n",
    "                            employees_to_skip_dd[employee_id] = {\n",
    "                                'reason': 'open_assignment_this_week',\n",
    "                                'content_id': content_id,\n",
    "                                'due_date': due_date_check\n",
    "                            }\n",
    "        \n",
    "        # Log employees who will be skipped for Daily Dose\n",
    "        if employees_to_skip_dd:\n",
    "            print(f\"\u274c SKIPPING {len(employees_to_skip_dd)} employee(s) - Daily Dose conflict:\")\n",
    "            print()\n",
    "            for emp_id in sorted(employees_to_skip_dd.keys()):\n",
    "                skip_info = employees_to_skip_dd[emp_id]\n",
    "                print(f\"  Employee {emp_id}:\")\n",
    "                \n",
    "                if skip_info['reason'] == 'completed_this_week':\n",
    "                    print(f\"    Reason: Already completed Daily Dose this week\")\n",
    "                    print(f\"    Completed Content: {skip_info['content_id']}\")\n",
    "                    print(f\"    Completion Date: {skip_info['completion_date']}\")\n",
    "                else:\n",
    "                    print(f\"    Reason: Has open Daily Dose assignment for this week\")\n",
    "                    print(f\"    Assigned Content: {skip_info['content_id']}\")\n",
    "                    print(f\"    Due Date: {skip_info['due_date']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"\u2713 No Daily Dose conflicts found\")\n",
    "            print()\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print()\n",
    "        \n",
    "        # Create Daily Dose assignments for eligible employees\n",
    "        eligible_employees_dd = [emp for _, emp in employees_df.iterrows() \n",
    "                                 if emp['employee_id'] not in employees_to_skip_dd]\n",
    "        \n",
    "        if len(eligible_employees_dd) > 0:\n",
    "            print(f\"\u2713 ASSIGNING DAILY DOSE TO {len(eligible_employees_dd)} ELIGIBLE EMPLOYEE(S)\")\n",
    "            print()\n",
    "            \n",
    "            for employee in eligible_employees_dd:\n",
    "                employee_id = employee['employee_id']\n",
    "                \n",
    "                print(f\"Employee {employee_id}:\")\n",
    "                \n",
    "                # Assign each selected Daily Dose content to this employee\n",
    "                for _, content in contents_to_assign.iterrows():\n",
    "                    content_id = content['ContentId']\n",
    "                    content_name = content['ContentName']\n",
    "                    \n",
    "                    print(f\"  \u2713 Daily Dose: {content_id} - {content_name}\")\n",
    "                    \n",
    "                    # Store what was assigned\n",
    "                    if employee_id not in employee_assigned_daily_dose:\n",
    "                        employee_assigned_daily_dose[employee_id] = []\n",
    "                    employee_assigned_daily_dose[employee_id].append({\n",
    "                        'content_id': content_id,\n",
    "                        'content_name': content_name\n",
    "                    })\n",
    "                    \n",
    "                    assignment = {\n",
    "                        \"UserID\": employee_id,\n",
    "                        \"CreateDate_text\": created_date,\n",
    "                        \"RequestId\": generate_request_id(),\n",
    "                        \"TrainingElementId\": content_id,\n",
    "                        \"Start_Date_text\": start_date,\n",
    "                        \"DueDate_text\": due_date,\n",
    "                        \"ContentType\": \"Media\"\n",
    "                    }\n",
    "                    \n",
    "                    new_manager_assignments.append(assignment)\n",
    "                \n",
    "                print()\n",
    "            \n",
    "            dd_count = len([a for a in new_manager_assignments])\n",
    "            print(f\"Created {dd_count} Daily Dose assignments\")\n",
    "            print()\n",
    "        else:\n",
    "            print(\"\u26a0 NO ELIGIBLE EMPLOYEES for Daily Dose\")\n",
    "            print(\"All employees have Daily Dose conflicts (completed or assigned this week).\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No Daily Dose content found.\")\n",
    "        print()\n",
    "    \n",
    "    # PART B: RANDOM NON-DAILY DOSE ASSIGNMENTS\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PART B: RANDOM NON-DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Filter for content where Daily_Dose_BA is NOT TRUE\n",
    "    print(\"Filtering for NON-Daily Dose training (Daily_Dose_BA != TRUE)...\")\n",
    "    non_daily_dose_content = standalone_df[\n",
    "        ~((standalone_df['Daily_Dose_BA'] == 'TRUE') | \n",
    "          (standalone_df['Daily_Dose_BA'] == True))\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Found {len(non_daily_dose_content)} non-Daily Dose content items\")\n",
    "    print()\n",
    "    \n",
    "    if len(non_daily_dose_content) > 0:\n",
    "        # Collect assignment data for table display\n",
    "        random_assignment_rows = []\n",
    "        \n",
    "        for _, employee in employees_df.iterrows():\n",
    "            employee_id = employee['employee_id']\n",
    "            \n",
    "            # Randomly select one content from non-Daily Dose content\n",
    "            selected_content = non_daily_dose_content.sample(n=1).iloc[0]\n",
    "            content_id = selected_content['ContentId']\n",
    "            content_name = selected_content['ContentName']\n",
    "            \n",
    "            # Store for table display\n",
    "            random_assignment_rows.append((employee_id, content_id, content_name))\n",
    "            \n",
    "            # Store what was assigned\n",
    "            employee_assigned_random[employee_id] = {\n",
    "                'content_id': content_id,\n",
    "                'content_name': content_name\n",
    "            }\n",
    "            \n",
    "            assignment = {\n",
    "                \"UserID\": employee_id,\n",
    "                \"CreateDate_text\": created_date,\n",
    "                \"RequestId\": generate_request_id(),\n",
    "                \"TrainingElementId\": content_id,\n",
    "                \"Start_Date_text\": start_date,\n",
    "                \"DueDate_text\": due_date,\n",
    "                \"ContentType\": \"Media\"\n",
    "            }\n",
    "            \n",
    "            new_manager_assignments.append(assignment)\n",
    "        \n",
    "        # Print header\n",
    "        print(f\"Random non-Daily Dose training assigned to {len(employees_df)} employee(s):\")\n",
    "        print()\n",
    "        print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Content Name'}\")\n",
    "        print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "        \n",
    "        # Print each assignment\n",
    "        for emp_id, content_id, content_name in sorted(random_assignment_rows):\n",
    "            print(f\"{emp_id:<15} | {content_id:<15} | {content_name}\")\n",
    "        \n",
    "        print()\n",
    "        random_count = len(employee_assigned_random)\n",
    "        print(f\"Created {random_count} random non-Daily Dose assignments\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No non-Daily Dose content found.\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Standalone content file not found.\")\n",
    "    print(\"Please run the preprocessing section first.\")\n",
    "\n",
    "# Step 3: Combine Databricks assignments with new manager assignments\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINING ASSIGNMENTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"  Assignments from Databricks Table: {len(databricks_assignments)}\")\n",
    "print(f\"  New Daily Dose assignments: {len(employee_assigned_daily_dose) * len(contents_to_assign) if employee_assigned_daily_dose else 0}\")\n",
    "print(f\"  New random Non-Daily Dose assignments: {len(employee_assigned_random)}\")\n",
    "print(f\"  Total new assignments: {len(new_manager_assignments)}\")\n",
    "\n",
    "# Databricks assignments go FIRST (as per manager.md)\n",
    "all_assignments = databricks_assignments + new_manager_assignments\n",
    "print(f\"  Total assignments for output: {len(all_assignments)}\")\n",
    "print()\n",
    "\n",
    "# Step 4: Generate output file\n",
    "if all_assignments:\n",
    "    assignments_filename = generate_non_completed_assignments_filename()\n",
    "    assignments_path = f\"{OUTPUT_DIR}/{assignments_filename}\"\n",
    "    \n",
    "    # Create DataFrame\n",
    "    assignments_df = pd.DataFrame(all_assignments)\n",
    "    \n",
    "    # Write to CSV with proper quoting\n",
    "    assignments_df.to_csv(assignments_path, index=False, quoting=1)  # quoting=1 means QUOTE_ALL\n",
    "    \n",
    "    print(f\"Generated NonCompletedAssignments file: {assignments_filename}\")\n",
    "    print(f\"  Databricks Table assignments: {len(databricks_assignments)}\")\n",
    "    print(f\"  New manager assignments: {len(new_manager_assignments)}\")\n",
    "    print(f\"  Total assignments in file: {len(all_assignments)}\")\n",
    "    print()\n",
    "    \n",
    "    # Print summary for NEW assignments\n",
    "    if new_manager_assignments:\n",
    "        print(\"New Assignment Summary:\")\n",
    "        if employee_assigned_daily_dose:\n",
    "            print(f\"  Daily Dose assignments: {len(employee_assigned_daily_dose)} employee(s) \u00d7 {len(contents_to_assign)} contents = {len(employee_assigned_daily_dose) * len(contents_to_assign)}\")\n",
    "            if employees_to_skip_dd:\n",
    "                print(f\"  Skipped (Daily Dose conflicts): {len(employees_to_skip_dd)} employee(s)\")\n",
    "        if employee_assigned_random:\n",
    "            print(f\"  Random assignments: {len(employee_assigned_random)} employee(s) \u00d7 1 content = {len(employee_assigned_random)}\")\n",
    "        print(f\"  Total new assignments: {len(new_manager_assignments)}\")\n",
    "        print(f\"  CreateDate: {created_date}\")\n",
    "        print(f\"  Start Date: {start_date}\")\n",
    "        print(f\"  Due Date: {due_date}\")\n",
    "else:\n",
    "    print(\"No assignments created (neither from Databricks nor new manager assignments).\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Training Simulation\n",
    "\n",
    "This section simulates employees completing training based on manager assignments and AI recommendations:\n",
    "\n",
    "## Workflow:\n",
    "1. **Get Recommendations** (next cell): Calls ML Training Recommender API for each employee\n",
    "2. **Get Manager Assignments**: Loads assignments from NonCompletedAssignments file created by manager\n",
    "3. **Combine Training**: Merges manager assignments with AI recommendations\n",
    "4. **Check Recent Completions**: Queries content_completion table for training completed in last 13 days\n",
    "   - **ONLY applies to AI recommendations** - manager assignments are ALWAYS included\n",
    "   - Employees skip AI-recommended training they completed recently (today + prior 12 days)\n",
    "   - Filtered AI training is removed from available training list\n",
    "   - Logs which AI training was skipped and why\n",
    "5. **Helper Functions** (following cell): Generates training timestamps\n",
    "6. **Process Employee**: Determines completions based on employee type:\n",
    "   - Type A: Completes all training (manager + filtered AI)\n",
    "   - Type B: Completes one training (from combined list of manager + filtered AI)\n",
    "   - Type F: Completes no training\n",
    "7. **Filename Generator**: Creates unique output filename with timestamp\n",
    "8. **Main Loop**: Processes all employees and collects completion records\n",
    "9. **Generate Output**: Writes ContentUserCompletion CSV file\n",
    "10. **Update NonCompletedAssignments**: Removes completed training from NonCompletedAssignments file\n",
    "11. **Print Summary**: Displays completion summary with source (manager or AI) for each employee\n",
    "\n",
    "**Important Notes**: \n",
    "- Employees will skip AI-recommended training they already completed in the last 13 days (current day + prior 12 days)\n",
    "- Manager-assigned training is NEVER skipped - employees always see and complete manager assignments regardless of recent completion history\n",
    "- This prevents duplicate AI recommendations while ensuring manager assignments are always honored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_recommendations(employee_id: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Call the training recommender API for a given employee.\n",
    "    \n",
    "    Args:\n",
    "        employee_id: The employee's ID (ba_id)\n",
    "    \n",
    "    Returns:\n",
    "        List of recommended training courses\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE_URL}{API_ENDPOINT}\"\n",
    "    payload = {\"data\": {\"ba_id\": employee_id}}\n",
    "    \n",
    "    try:\n",
    "        # Disable SSL certificate verification for internal APIs\n",
    "        response = requests.post(url, json=payload, timeout=API_TIMEOUT, verify=False)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Response structure: {\"response\": {\"ml_recommendations\": [...], \"coaching_note\": {...}}, \"timing\": {...}, \"apiContext\": {...}}\n",
    "        if isinstance(data, dict):\n",
    "            response_data = data.get(\"response\", {})\n",
    "            if isinstance(response_data, dict):\n",
    "                # Get ml_recommendations from nested response\n",
    "                recommendations = response_data.get(\"ml_recommendations\", [])\n",
    "            else:\n",
    "                # Response is directly a list\n",
    "                recommendations = response_data if isinstance(response_data, list) else []\n",
    "        else:\n",
    "            print(f\"  Unexpected response type: {type(data)}\")\n",
    "            return []\n",
    "        \n",
    "        # Print selected fields from API response\n",
    "        if isinstance(recommendations, list) and recommendations:\n",
    "            print(f\"  ML Recommendation API Response for employee {employee_id}:\")\n",
    "            for rec in recommendations:\n",
    "                content_id = rec.get(\"recommended_content_id\", \"N/A\")\n",
    "                recommended_content = rec.get(\"recommended_content\", \"N/A\")\n",
    "                print(f\"  {content_id} | {recommended_content}\")\n",
    "            print()\n",
    "        \n",
    "        # Ensure we have a list\n",
    "        if isinstance(recommendations, list):\n",
    "            return recommendations\n",
    "        else:\n",
    "            print(f\"  Recommendations is not a list: {type(recommendations)}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching recommendations for employee {employee_id}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_times(num_courses: int) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Generate start and completion times for training courses.\n",
    "    Calculates times in PT timezone (00:05 and 00:09), then converts to UTC for output.\n",
    "    \n",
    "    All timestamps are returned in ISO-8601 format with UTC timezone offset (+00:00).\n",
    "    \n",
    "    Args:\n",
    "        num_courses: Number of courses to generate times for\n",
    "    \n",
    "    Returns:\n",
    "        List of (start_time, end_time) tuples in ISO-8601 format with UTC timezone\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    now = datetime.now(PT)  # Use PT timezone for calculation\n",
    "    \n",
    "    # Set to current day at 13:15 PT for start time\n",
    "    start_time_pt = now.replace(hour=13, minute=15, second=0, microsecond=0)\n",
    "    \n",
    "    # Set to current day at 13:19 PT for completion time\n",
    "    end_time_pt = now.replace(hour=13, minute=19, second=0, microsecond=0)\n",
    "    \n",
    "    # Convert to UTC before formatting\n",
    "    start_time_utc = start_time_pt.astimezone(UTC)\n",
    "    end_time_utc = end_time_pt.astimezone(UTC)\n",
    "    \n",
    "    for _ in range(num_courses):\n",
    "        # Return ISO-8601 format with UTC offset\n",
    "        times.append((\n",
    "            start_time_utc.isoformat(),\n",
    "            end_time_utc.isoformat()\n",
    "        ))\n",
    "    \n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_employee_recent_completions(employee_id: int, lookback_days: int = 13) -> set:\n",
    "    \"\"\"\n",
    "    Query content_completion table to get training completed by employee in the last N days.\n",
    "    \n",
    "    Args:\n",
    "        employee_id: The employee's ID (ba_id)\n",
    "        lookback_days: Number of days to look back (default: 13 = today + prior 12 days)\n",
    "    \n",
    "    Returns:\n",
    "        Set of content IDs completed in the lookback period\n",
    "    \"\"\"\n",
    "    # Check if Databricks is configured\n",
    "    if not all([DATABRICKS_HOST, DATABRICKS_HTTP_PATH, DATABRICKS_TOKEN]):\n",
    "        # Databricks not configured - return empty set (no skip logic)\n",
    "        return set()\n",
    "    \n",
    "    try:\n",
    "        from databricks import sql\n",
    "        \n",
    "        # Connect to Databricks\n",
    "        connection = sql.connect(\n",
    "            server_hostname=DATABRICKS_HOST,\n",
    "            http_path=DATABRICKS_HTTP_PATH,\n",
    "            access_token=DATABRICKS_TOKEN\n",
    "        )\n",
    "        \n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Table name\n",
    "        completion_table = f\"{DATABRICKS_CATALOG}.{DATABRICKS_SCHEMA}.content_completion\"\n",
    "        \n",
    "        # Calculate date range in PT timezone\n",
    "        now_pt = datetime.now(PT)\n",
    "        start_date = (now_pt - timedelta(days=lookback_days - 1)).date()  # -1 because today is included\n",
    "        end_date = now_pt.date()\n",
    "        \n",
    "        # Query: Find all content completed by this employee in the last N days\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT content_id\n",
    "        FROM {completion_table}\n",
    "        WHERE ba_id = {employee_id}\n",
    "            AND completion_date >= '{start_date}'\n",
    "            AND completion_date <= '{end_date}'\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # Fetch results\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        \n",
    "        # Build set of content IDs\n",
    "        recent_content_ids = set()\n",
    "        for row in rows:\n",
    "            content_id = int(row[0])\n",
    "            recent_content_ids.add(content_id)\n",
    "        \n",
    "        return recent_content_ids\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  \u26a0 Could not query recent completions for employee {employee_id}: {e}\")\n",
    "        print(f\"  Continuing without skip logic...\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def process_employee(employee_id: int, employee_type: str, manager_assignments_path: str, standalone_df: pd.DataFrame, ai_recommendations: List[Dict] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single employee: get AI recommendations and manager assignments, then simulate completions.\n",
    "\n",
    "    Args:\n",
    "        employee_id: The employee's ID\n",
    "        employee_type: The employee's type (a, b, or f)\n",
    "        manager_assignments_path: Path to the NonCompletedAssignments CSV file\n",
    "        standalone_df: DataFrame containing standalone content for lookups\n",
    "        ai_recommendations: Optional pre-fetched AI recommendations (to avoid duplicate API calls)\n",
    "\n",
    "    Returns:\n",
    "        List of completed training records with PT timezone timestamps\n",
    "    \"\"\"\n",
    "    employee_type = employee_type.lower().strip()\n",
    "\n",
    "    # Get AI recommendations (use provided ones or fetch new)\n",
    "    if ai_recommendations is None:\n",
    "        ai_recommendations = get_training_recommendations(employee_id)\n",
    "\n",
    "    # Get manager assignments\n",
    "    manager_assignments = []\n",
    "    if os.path.exists(manager_assignments_path):\n",
    "        assignments_df = pd.read_csv(manager_assignments_path)\n",
    "        \n",
    "        # Convert UserID to int to match employee_id type\n",
    "        # (CSV with QUOTE_ALL reads as string, but employee_id is int)\n",
    "        assignments_df['UserID'] = assignments_df['UserID'].astype(int)\n",
    "        \n",
    "        # Filter for this employee\n",
    "        employee_assignments = assignments_df[assignments_df['UserID'] == employee_id]\n",
    "\n",
    "        for _, assignment in employee_assignments.iterrows():\n",
    "            # Get the TrainingElementId and look up the content name\n",
    "            content_id = assignment['TrainingElementId']\n",
    "\n",
    "            # Remove commas from content_id if present (it might be formatted)\n",
    "            if isinstance(content_id, str):\n",
    "                content_id_numeric = int(content_id.replace(',', ''))\n",
    "            else:\n",
    "                content_id_numeric = int(content_id)\n",
    "\n",
    "            # Look up content name in standalone_df\n",
    "            # Handle both numeric and string ContentId in standalone_df\n",
    "            content_row = standalone_df[\n",
    "                (standalone_df['ContentId'] == content_id) |\n",
    "                (standalone_df['ContentId'] == str(content_id_numeric))\n",
    "            ]\n",
    "            if not content_row.empty:\n",
    "                content_name = content_row.iloc[0]['ContentName']\n",
    "            else:\n",
    "                content_name = \"Unknown Manager Assignment\"\n",
    "\n",
    "            manager_assignments.append({\n",
    "                \"recommended_content_id\": content_id_numeric,\n",
    "                \"recommended_content\": content_name,\n",
    "                \"source\": \"manager\"\n",
    "            })\n",
    "\n",
    "    # Tag AI recommendations with source\n",
    "    for rec in ai_recommendations:\n",
    "        rec[\"source\"] = \"ai\"\n",
    "\n",
    "    # NEW: Check for recently completed training (last 13 days)\n",
    "    # This ONLY applies to AI recommendations, NOT to manager assignments\n",
    "    recent_completions = get_employee_recent_completions(employee_id, lookback_days=13)\n",
    "    \n",
    "    filtered_ai_recommendations = []\n",
    "    \n",
    "    if ai_recommendations:\n",
    "        if recent_completions:\n",
    "            print(f\"  Recent completions (last 13 days): {len(recent_completions)} content(s)\")\n",
    "            print(f\"  Filtering AI recommendations for recent completions...\")\n",
    "            \n",
    "            skipped_ai_training = []\n",
    "            \n",
    "            for rec in ai_recommendations:\n",
    "                content_id = rec[\"recommended_content_id\"]\n",
    "                \n",
    "                if content_id in recent_completions:\n",
    "                    # Skip this AI recommendation - completed recently\n",
    "                    skipped_ai_training.append(rec)\n",
    "                    print(f\"    \u2298 IGNORING (ML): {format_content_id(content_id)} - {rec['recommended_content'][:50]}\")\n",
    "                    print(f\"      Reason: Employee Completed in last 13 days\")\n",
    "                else:\n",
    "                    # Keep this AI recommendation - not completed recently\n",
    "                    filtered_ai_recommendations.append(rec)\n",
    "            \n",
    "            if skipped_ai_training:\n",
    "                print(f\"  Skipped {len(skipped_ai_training)} AI recommendation(s) due to recent completion\")\n",
    "                print(f\"  AI recommendations after skip filter: {len(filtered_ai_recommendations)}\")\n",
    "        else:\n",
    "            # No recent completions - keep all AI recommendations\n",
    "            filtered_ai_recommendations = ai_recommendations\n",
    "    \n",
    "    # Combine manager assignments (unfiltered) with filtered AI recommendations\n",
    "    # Manager assignments are ALWAYS included - no skip logic for manager assignments\n",
    "    all_training = manager_assignments + filtered_ai_recommendations\n",
    "\n",
    "    if not all_training:\n",
    "        print(f\"  No training available for employee {employee_id}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"  Total training available: {len(all_training)} ({len(manager_assignments)} manager + {len(filtered_ai_recommendations)} AI)\")\n",
    "\n",
    "    # Determine how many courses to complete based on employee type\n",
    "    if employee_type == 'a':\n",
    "        # Type A: complete all training (manager + AI)\n",
    "        courses_to_complete = all_training\n",
    "    elif employee_type == 'b':\n",
    "        # Type B: complete one training (from combined list)\n",
    "        courses_to_complete = all_training[:1]\n",
    "    else:\n",
    "        # Type F: complete no training\n",
    "        courses_to_complete = []\n",
    "\n",
    "    # Generate completion records with PT timezone timestamps\n",
    "    completions = []\n",
    "    times = generate_training_times(len(courses_to_complete))\n",
    "\n",
    "    for i, course in enumerate(courses_to_complete):\n",
    "        try:\n",
    "            # Validate course is a dict\n",
    "            if not isinstance(course, dict):\n",
    "                print(f\"  WARNING: Course is not a dict, it's {type(course)}: {course}\")\n",
    "                continue\n",
    "\n",
    "            start_time, end_time = times[i]\n",
    "            source = course.get(\"source\", \"unknown\")\n",
    "            completions.append({\n",
    "                \"UserId\": employee_id,\n",
    "                \"ContentId\": format_content_id(course[\"recommended_content_id\"]),\n",
    "                \"DateStarted\": start_time,  # ISO-8601 with PT timezone\n",
    "                \"DateCompleted\": end_time,  # ISO-8601 with PT timezone\n",
    "                \"CourseName\": course.get(\"recommended_content\", \"Unknown\"),\n",
    "                \"Source\": source\n",
    "            })\n",
    "        except KeyError as e:\n",
    "            print(f\"  WARNING: Missing key {e} in course data: {course}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Error processing course: {e}\")\n",
    "            continue\n",
    "\n",
    "    return completions\n",
    "\n",
    "def generate_output_filename() -> str:\n",
    "    \"\"\"\n",
    "    Generate output filename with PT timestamp.\n",
    "    Format: ContentUserCompletion_V2_YYYY_MM_DD_1_HHMMSS.csv\n",
    "    Uses PT timezone for date and time components for consistency.\n",
    "    \n",
    "    Returns:\n",
    "        Generated filename\n",
    "    \"\"\"\n",
    "    now = datetime.now(PT)  # Use PT timezone for consistency\n",
    "    year = now.strftime(\"%Y\")\n",
    "    month = now.strftime(\"%m\")\n",
    "    day = now.strftime(\"%d\")\n",
    "    \n",
    "    # Generate 6-digit time suffix: HHMMSS\n",
    "    time_suffix = now.strftime(\"%H%M%S\")\n",
    "    \n",
    "    return f\"ContentUserCompletion_V2_{year}_{month}_{day}_1_{time_suffix}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution - Process employees and simulate training completions\n",
    "print(\"=\" * 80)\n",
    "print(\"EMPLOYEE TRAINING SIMULATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check if manager assignments were created\n",
    "if 'assignments_path' not in locals() or not os.path.exists(assignments_path):\n",
    "    print(\"WARNING: Manager assignments file not found. Employees will only complete AI recommendations.\")\n",
    "    print()\n",
    "    assignments_path = \"\"\n",
    "\n",
    "# Process each employee\n",
    "all_completions = []\n",
    "employee_summaries = []\n",
    "employee_ml_recommendations = []  # Store ML recommendations for summary\n",
    "\n",
    "for _, employee in employees_df.iterrows():\n",
    "    employee_id = employee['employee_id']\n",
    "    employee_type = employee['employee_edu_type']\n",
    "    \n",
    "    print(f\"Processing Employee {employee_id} (Type {employee_type.upper()})...\")\n",
    "    \n",
    "    # Get AI recommendations\n",
    "    ai_recommendations = get_training_recommendations(employee_id)\n",
    "    \n",
    "    # Store ML recommendations for this employee\n",
    "    if ai_recommendations:\n",
    "        ml_recs = []\n",
    "        for rec in ai_recommendations:\n",
    "            ml_recs.append({\n",
    "                \"content_id\": rec.get(\"recommended_content_id\"),\n",
    "                \"content_name\": rec.get(\"recommended_content\", \"Unknown\")\n",
    "            })\n",
    "        employee_ml_recommendations.append((employee_id, ml_recs))\n",
    "    \n",
    "    # Process employee with pre-fetched AI recommendations\n",
    "    completions = process_employee(employee_id, employee_type, assignments_path, standalone_df, ai_recommendations)\n",
    "    \n",
    "    if completions:\n",
    "        all_completions.extend(completions)\n",
    "        # Store ContentId, CourseName, and Source for summary\n",
    "        course_details = [(c['ContentId'], c['CourseName'], c['Source']) for c in completions]\n",
    "        employee_summaries.append((employee_id, course_details))\n",
    "        print(f\"  Completed {len(completions)} training(s)\")\n",
    "    else:\n",
    "        print(f\"  No training completed\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output file\n",
    "if all_completions:\n",
    "    output_filename = generate_output_filename()\n",
    "    output_path = f\"{OUTPUT_DIR}/{output_filename}\"\n",
    "    \n",
    "    # Create DataFrame with only the required columns for CSV\n",
    "    output_df = pd.DataFrame(all_completions)\n",
    "    output_df = output_df[['UserId', 'ContentId', 'DateStarted', 'DateCompleted']]\n",
    "    \n",
    "    # Write to CSV with proper quoting\n",
    "    output_df.to_csv(output_path, index=False, quoting=1)  # quoting=1 means QUOTE_ALL\n",
    "    \n",
    "    print(f\"Generated output file: {output_filename}\")\n",
    "    print(f\"Total completions: {len(all_completions)}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"No training completions to write.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update NonCompletedAssignments file to remove completed training\n",
    "if all_completions and 'assignments_path' in locals() and os.path.exists(assignments_path):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"UPDATING NON-COMPLETED ASSIGNMENTS FILE\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Read current assignments file\n",
    "    print(f\"Reading assignments from: {assignments_path}\")\n",
    "    assignments_df = pd.read_csv(assignments_path)\n",
    "    initial_count = len(assignments_df)\n",
    "    print(f\"  Initial assignments: {initial_count}\")\n",
    "    print()\n",
    "    \n",
    "    # Build set of completed (UserID, ContentID) pairs\n",
    "    # ContentID in completions has commas, ContentID in assignments might also have commas\n",
    "    completed_set = set()\n",
    "    \n",
    "    for completion in all_completions:\n",
    "        user_id = completion['UserId']\n",
    "        content_id = completion['ContentId']\n",
    "        # Normalize content_id by removing commas for comparison\n",
    "        if isinstance(content_id, str):\n",
    "            content_id_numeric = int(content_id.replace(',', ''))\n",
    "        else:\n",
    "            content_id_numeric = int(content_id)\n",
    "        completed_set.add((user_id, content_id_numeric))\n",
    "    \n",
    "    print(f\"Completed training count: {len(completed_set)}\")\n",
    "    print()\n",
    "    \n",
    "    # Filter out completed assignments\n",
    "    removed_assignments = []  # Track what we remove for summary\n",
    "    \n",
    "    def is_not_completed(row):\n",
    "        \"\"\"Check if an assignment was NOT completed\"\"\"\n",
    "        user_id = int(row['UserID'])\n",
    "        training_id = row['TrainingElementId']\n",
    "        \n",
    "        # Normalize training_id by removing commas\n",
    "        if isinstance(training_id, str):\n",
    "            training_id_numeric = int(training_id.replace(',', ''))\n",
    "        else:\n",
    "            training_id_numeric = int(training_id)\n",
    "        \n",
    "        # Return True if NOT in completed set (keep assignment)\n",
    "        is_completed = (user_id, training_id_numeric) in completed_set\n",
    "        if is_completed:\n",
    "            removed_assignments.append((user_id, training_id_numeric, training_id))\n",
    "        return not is_completed\n",
    "    \n",
    "    # Apply filter\n",
    "    remaining_assignments_df = assignments_df[assignments_df.apply(is_not_completed, axis=1)].copy()\n",
    "    \n",
    "    removed_count = initial_count - len(remaining_assignments_df)\n",
    "    \n",
    "    print(f\"Assignments breakdown:\")\n",
    "    print(f\"  Initial assignments: {initial_count}\")\n",
    "    print(f\"  Completed assignments (removed): {removed_count}\")\n",
    "    print(f\"  Remaining assignments: {len(remaining_assignments_df)}\")\n",
    "    print()\n",
    "    \n",
    "    # Show sample of removed assignments\n",
    "    if removed_assignments:\n",
    "        print(f\"Sample of removed assignments (first 5):\")\n",
    "        for i, (uid, cid, original_id) in enumerate(removed_assignments[:5]):\n",
    "            print(f\"  {i+1}. Employee {uid}, Content {cid} (original: '{original_id}')\")\n",
    "        if len(removed_assignments) > 5:\n",
    "            print(f\"  ... and {len(removed_assignments) - 5} more\")\n",
    "        print()\n",
    "    \n",
    "    # Overwrite the file with remaining assignments\n",
    "    if len(remaining_assignments_df) > 0:\n",
    "        remaining_assignments_df.to_csv(assignments_path, index=False, quoting=1)\n",
    "        print(f\"\u2713 Updated NonCompletedAssignments file\")\n",
    "        print(f\"  File: {assignments_path}\")\n",
    "        print(f\"  Removed {removed_count} completed assignment(s)\")\n",
    "        print(f\"  {len(remaining_assignments_df)} assignment(s) remain\")\n",
    "    else:\n",
    "        # All assignments were completed - create empty file with headers\n",
    "        remaining_assignments_df.to_csv(assignments_path, index=False, quoting=1)\n",
    "        print(f\"\u2713 All assignments completed!\")\n",
    "        print(f\"  File updated with headers only (no remaining assignments)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "elif all_completions:\n",
    "    print(\"\u26a0 NonCompletedAssignments file not found - skipping update\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"\u26a0 No completions to process - skipping NonCompletedAssignments update\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"-\" * 80)\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MANAGER-ASSIGNMENTS GIVEN NEW\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# PART A: Daily Dose Assignments\n",
    "if 'employee_assigned_daily_dose' in locals() and len(employee_assigned_daily_dose) > 0:\n",
    "    print(\"PART A: DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    print(f\"Daily Dose training assigned to {len(employee_assigned_daily_dose)} employee(s):\")\n",
    "    print()\n",
    "    \n",
    "    # Get the contents from the first employee (they all have the same Daily Dose)\n",
    "    first_employee_contents = list(employee_assigned_daily_dose.values())[0]\n",
    "    \n",
    "    print(\"Daily Dose Contents:\")\n",
    "    for content_info in first_employee_contents:\n",
    "        content_id = content_info['content_id']\n",
    "        content_name = content_info['content_name']\n",
    "        print(f\"  {content_id} - {content_name}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Print list of all employees who received these assignments\n",
    "    employee_ids_str = \", \".join([str(emp_id) for emp_id in sorted(employee_assigned_daily_dose.keys())])\n",
    "    print(f\"Employees: {employee_ids_str}\")\n",
    "    print()\n",
    "    \n",
    "    if 'employees_to_skip_dd' in locals() and employees_to_skip_dd:\n",
    "        skipped_ids_str = \", \".join([str(emp_id) for emp_id in sorted(employees_to_skip_dd.keys())])\n",
    "        print(f\"Skipped (already have Daily Dose): {skipped_ids_str}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"PART A: DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    print(\"No new Daily Dose assignments were created in this run.\")\n",
    "    if 'employees_to_skip_dd' in locals() and employees_to_skip_dd:\n",
    "        print(f\"All {len(employees_to_skip_dd)} employee(s) already have Daily Dose for current week.\")\n",
    "    print()\n",
    "\n",
    "# PART B: Random Non-Daily Dose Assignments\n",
    "if 'employee_assigned_random' in locals() and len(employee_assigned_random) > 0:\n",
    "    print(\"PART B: RANDOM NON-DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    print(f\"Random non-Daily Dose training assigned to {len(employee_assigned_random)} employee(s):\")\n",
    "    print()\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Course Name'}\")\n",
    "    print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "    \n",
    "    # Print each employee's random assignment\n",
    "    for emp_id in sorted(employee_assigned_random.keys()):\n",
    "        content_info = employee_assigned_random[emp_id]\n",
    "        content_id = content_info['content_id']\n",
    "        content_name = content_info['content_name']\n",
    "        print(f\"{emp_id:<15} | {content_id:<15} | {content_name}\")\n",
    "    \n",
    "    print()\n",
    "else:\n",
    "    print(\"PART B: NON-DAILY DOSE ASSIGNMENTS RAMDOMLY CHOSEN\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    print(\"No random non-Daily Dose assignments were created in this run.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RECOMMENDATIONS GIVEN BY ML API\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Display all ML recommendations given to employees in pipe-separated format\n",
    "if employee_ml_recommendations:\n",
    "    # Collect all recommendation rows\n",
    "    recommendation_rows = []\n",
    "    \n",
    "    for employee_id, ml_recs in employee_ml_recommendations:\n",
    "        for rec in ml_recs:\n",
    "            content_id = rec[\"content_id\"]\n",
    "            content_name = rec[\"content_name\"]\n",
    "            recommendation_rows.append((employee_id, content_id, content_name))\n",
    "    \n",
    "    # Sort by employee ID, then content ID\n",
    "    recommendation_rows.sort(key=lambda x: (x[0], str(x[1])))\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Content Name'}\")\n",
    "    print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "    \n",
    "    # Print each recommendation as a separate row\n",
    "    for employee_id, content_id, content_name in recommendation_rows:\n",
    "        print(f\"{employee_id:<15} | {content_id:<15} | {content_name}\")\n",
    "    \n",
    "    print()\n",
    "else:\n",
    "    print(\"No ML recommendations were given to any employee.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EMPLOYEE TRAINING COMPLETIONS OF MANAGER-ASSIGNED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Track if any manager assignments were completed\n",
    "manager_completions_found = False\n",
    "\n",
    "# Collect all manager completions for table display\n",
    "manager_completion_rows = []\n",
    "\n",
    "for employee_id, course_details in employee_summaries:\n",
    "    # Filter for manager-assigned training only\n",
    "    manager_courses = [(content_id, course_name) for content_id, course_name, source in course_details if source == \"manager\"]\n",
    "    \n",
    "    if manager_courses:\n",
    "        manager_completions_found = True\n",
    "        for content_id, course_name in manager_courses:\n",
    "            manager_completion_rows.append((employee_id, content_id, course_name))\n",
    "\n",
    "if manager_completions_found:\n",
    "    # Sort by employee ID, then content ID\n",
    "    manager_completion_rows.sort(key=lambda x: (x[0], str(x[1])))\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Course Name'}\")\n",
    "    print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "    \n",
    "    # Print each completion on a separate row\n",
    "    for employee_id, content_id, course_name in manager_completion_rows:\n",
    "        print(f\"{employee_id:<15} | {content_id:<15} | {course_name}\")\n",
    "else:\n",
    "    print(\"No manager-assigned training was completed by any employee.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\" EMPLOYEE TRAINING COMPLETIONS OF ML-RECOMMENDED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Track if any ML recommendations were completed\n",
    "ml_completions_found = False\n",
    "\n",
    "# Collect all ML completions for table display\n",
    "ml_completion_rows = []\n",
    "\n",
    "for employee_id, course_details in employee_summaries:\n",
    "    # Filter for ML-recommended training only\n",
    "    ml_courses = [(content_id, course_name) for content_id, course_name, source in course_details if source == \"ai\"]\n",
    "    \n",
    "    if ml_courses:\n",
    "        ml_completions_found = True\n",
    "        for content_id, course_name in ml_courses:\n",
    "            ml_completion_rows.append((employee_id, content_id, course_name))\n",
    "\n",
    "if ml_completions_found:\n",
    "    # Sort by employee ID, then content ID\n",
    "    ml_completion_rows.sort(key=lambda x: (x[0], str(x[1])))\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Course Name'}\")\n",
    "    print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "    \n",
    "    # Print each completion on a separate row\n",
    "    for employee_id, content_id, course_name in ml_completion_rows:\n",
    "        print(f\"{employee_id:<15} | {content_id:<15} | {course_name}\")\n",
    "else:\n",
    "    print(\"No ML-recommended training was completed by any employee.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"execution complete\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing - Publish generated files to SFTP outbound server\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"POSTPROCESSING - Publish Files to SFTP Outbound Server\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check if publishing is enabled\n",
    "if not SFTP_PUBLISH_ENABLED:\n",
    "    print(\"\u2298 SFTP publishing is DISABLED\")\n",
    "    print(f\"  To enable publishing, set SFTP_PUBLISH_ENABLED=true in .env file\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"\u2713 SFTP publishing is ENABLED\")\n",
    "    print()\n",
    "    \n",
    "    # Collect all generated files to publish\n",
    "    files_to_publish = []\n",
    "    \n",
    "    # Add ContentUserCompletion file\n",
    "    if 'output_path' in locals() and os.path.exists(output_path):\n",
    "        files_to_publish.append(output_path)\n",
    "    \n",
    "    # Add NonCompletedAssignments file\n",
    "    if 'assignments_path' in locals() and os.path.exists(assignments_path):\n",
    "        files_to_publish.append(assignments_path)\n",
    "    \n",
    "    # Add UserCompletion file\n",
    "    if 'user_completion_path' in locals() and os.path.exists(user_completion_path):\n",
    "        files_to_publish.append(user_completion_path)\n",
    "    \n",
    "    if files_to_publish:\n",
    "        print(f\"Files to publish ({len(files_to_publish)}):\")\n",
    "        for file_path in files_to_publish:\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"  - {filename}\")\n",
    "        print()\n",
    "        \n",
    "        # Publish files\n",
    "        print(\"Publishing files...\")\n",
    "        print(\"-\" * 80)\n",
    "        success = publish_files_to_sftp_outbound(files_to_publish)\n",
    "        print(\"-\" * 80)\n",
    "        print()\n",
    "        \n",
    "        if success:\n",
    "            print(\"\u2713 All files published successfully\")\n",
    "        else:\n",
    "            print(\"\u26a0 Some files failed to publish\")\n",
    "    else:\n",
    "        print(\"\u26a0 No files found to publish\")\n",
    "        print(\"  Generated files may not exist. Please run the notebook cells in order.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}