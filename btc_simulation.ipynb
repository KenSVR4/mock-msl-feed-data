{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTC Fake - Training Completion Simulator\n",
    "\n",
    "This notebook creates a set of files just like files sent by BTC vendor. The files convey\n",
    "the training Course Catalog, the transactions of managers assigning training to employees, and transactions of employees completing training. Employees can complete those manager-assigned trainings, PLUS training recommended by the ML Recommendation API.\n",
    "\n",
    "## How it works:\n",
    "1. **Preprocessing**: Downloads two files that BTC sent to the Prod SFTP server - the files that represent Courses and Contents. \n",
    "2. **Manager Assigns Training**: \n",
    "   - Queries content_assignments AND content_completion tables from Databricks in order to calculate open assignments (assignments - completions) for each employee\n",
    "   - Selects and assigns up to 3 Daily Dose contents for the current week to all employees\n",
    "3. **Employee Completes Training**: \n",
    "   - Possibley completes manager assignments and AI recommendations for each employee\n",
    "   - employee type (A, B, or F) determines if they complete all, one, or zero trainings.\n",
    "4. **Output File Generation**:\n",
    "   - NonCompletedAssignments CSV (open assignments from Databricks + new manager assignments)\n",
    "   - ContentUserCompletion CSV (completed training)\n",
    "   - UserCompletion CSV that is a dummy file required but not important here\n",
    "5. **Update NonCompletedAssignments**: Removes completed assignments, regenerates the file\n",
    "6. **Summary**: Prints run details showing training from manager vs AI and completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import string\n",
    "from typing import List, Dict\n",
    "import urllib3\n",
    "import pytz\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Import shared business logic\n",
    "import simulation_core as core\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Disable SSL warnings when ignoring certificate verification\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Define Pacific timezone globally for all timestamp calculations\n",
    "# Timestamps are calculated in PT for proper local time logic,\n",
    "# then converted to UTC (offset +00:00) for CSV output files\n",
    "PT = pytz.timezone('America/Los_Angeles')\n",
    "UTC = pytz.UTC\n",
    "\n",
    "# Load configuration from environment using simulation_core\n",
    "config = core.load_config()\n",
    "\n",
    "# Extract commonly used config values for backward compatibility\n",
    "API_BASE_URL = config['api_base_url']\n",
    "API_ENDPOINT = config['api_endpoint']\n",
    "API_TIMEOUT = config['api_timeout']\n",
    "\n",
    "EMPLOYEES_FILE = config['employees_file']\n",
    "OUTPUT_DIR = config['output_dir']\n",
    "SFTP_LOCAL_DIR = config['sftp_local_dir']\n",
    "USER_COMPLETION_TEMPLATE_FILE = config['user_completion_template_file']\n",
    "\n",
    "DATABRICKS_HOST = config['databricks_host']\n",
    "DATABRICKS_HTTP_PATH = config['databricks_http_path']\n",
    "DATABRICKS_TOKEN = config['databricks_token']\n",
    "DATABRICKS_CATALOG = config['databricks_catalog']\n",
    "DATABRICKS_SCHEMA = config['databricks_schema']\n",
    "\n",
    "SFTP_OUTBOUND_HOST = config['sftp_outbound_host']\n",
    "SFTP_OUTBOUND_USER = config['sftp_outbound_user']\n",
    "SFTP_OUTBOUND_PASSWORD = config['sftp_outbound_password']\n",
    "SFTP_OUTBOUND_REMOTE_PATH = config['sftp_outbound_remote_path']\n",
    "SFTP_PUBLISH_ENABLED = config['sftp_publish_enabled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - Download Files from SFTP\n",
    "\n",
    "This section prepares for a fresh run:\n",
    "\n",
    "## Cleanup\n",
    "1. Removes prior files from `generated_files/` directory to ensure each run starts with a clean slate\n",
    "\n",
    "## Generate a Dummy (not important) UserCompletion File\n",
    "1. Relies on the UserCompletion template in `docs/sample_files/` places new ne in `generated_files/` directory\n",
    "\n",
    "## Download Files (from SFTP Server) that BTC Vendor sent \n",
    "1. **CourseCatalog** - Training curriculum elements like Courses and components\n",
    "2. **StandAloneContent** - All training content (videos, PDFs, documents)\n",
    "\n",
    "## Requirements:\n",
    "1. Copy `.env.example` to `.env` and add your SFTP password\n",
    "2. Files will be downloaded to `downloaded_files/` directory\n",
    "3. The system finds the most recent file based on the date in the filename\n",
    "\n",
    "## File Formats:\n",
    "- CourseCatalog: `CourseCatalog_V2_YYYY_M_DD_1_random.csv`\n",
    "- StandAloneContent: `StandAloneContent_v2_YYYY_M_DD_1_random.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Remove old files from previous runs using simulation_core\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING - Cleanup\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Cleaning up directories from previous runs...\")\n",
    "print()\n",
    "\n",
    "# Clean generated_files directory using simulation_core function\n",
    "print(f\"Cleaning {OUTPUT_DIR}/...\")\n",
    "core.cleanup_output_directory(config, lambda msg: print(f\"  {msg}\"))\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate UserCompletion file from template using simulation_core\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING - Generate UserCompletion File\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Generating a dummy UserCompletion file from sample file...\")\n",
    "user_completion_path = core.generate_user_completion_file_from_template(config, print)\n",
    "\n",
    "if user_completion_path:\n",
    "    print(f\"✓ UserCompletion file generated successfully\")\n",
    "    print(f\"  File: {user_completion_path}\")\n",
    "else:\n",
    "    print(\"✗ Failed to generate UserCompletion file\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SFTP libraries and load environment\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import paramiko\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# SFTP Inbound Server Configuration (from config)\n",
    "SFTP_INBOUND_HOST = config['sftp_inbound_host']\n",
    "SFTP_INBOUND_USER = config['sftp_inbound_user']\n",
    "SFTP_INBOUND_PASSWORD = config['sftp_inbound_password']\n",
    "SFTP_INBOUND_REMOTE_PATH = config['sftp_inbound_remote_path']\n",
    "\n",
    "# Use simulation_core functions for SFTP operations\n",
    "parse_course_catalog_filename = core.parse_course_catalog_filename\n",
    "parse_standalone_content_filename = core.parse_standalone_content_filename\n",
    "\n",
    "def download_most_recent_course_catalog() -> str:\n",
    "    \"\"\"\n",
    "    Connect to SFTP inbound server and download the most recent CourseCatalog file.\n",
    "    Uses simulation_core for the actual download logic.\n",
    "    \n",
    "    Returns:\n",
    "        Path to the downloaded file, or None if download fails\n",
    "    \"\"\"\n",
    "    return core.download_most_recent_file_from_sftp(config, 'course_catalog')\n",
    "\n",
    "def download_most_recent_standalone_content() -> str:\n",
    "    \"\"\"\n",
    "    Connect to SFTP inbound server and download the most recent StandAloneContent file.\n",
    "    Uses simulation_core for the actual download logic.\n",
    "    \n",
    "    Returns:\n",
    "        Path to the downloaded file, or None if download fails\n",
    "    \"\"\"\n",
    "    return core.download_most_recent_file_from_sftp(config, 'standalone_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_files_to_sftp_outbound(files_to_publish: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Publish generated files to SFTP outbound server.\n",
    "    Uses simulation_core for the actual publish logic.\n",
    "    \n",
    "    Args:\n",
    "        files_to_publish: List of local file paths to upload\n",
    "    \n",
    "    Returns:\n",
    "        True if all files published successfully, False otherwise\n",
    "    \"\"\"\n",
    "    return core.publish_files_to_sftp_outbound(config, files_to_publish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute: Download Course Catalog and Standalone Content from SFTP\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING - Download Files from SFTP Inbound Server\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Download Course Catalog\n",
    "print(\"Downloading Course Catalog...\")\n",
    "print(\"-\" * 80)\n",
    "course_catalog_path = download_most_recent_course_catalog()\n",
    "\n",
    "if course_catalog_path:\n",
    "    print()\n",
    "    print(f\"✓ Course catalog downloaded successfully\")\n",
    "    print(f\"  File: {course_catalog_path}\")\n",
    "    \n",
    "    # Optionally load and preview the file\n",
    "    try:\n",
    "        catalog_df = pd.read_csv(course_catalog_path)\n",
    "        print(f\"  Rows: {len(catalog_df)}\")\n",
    "        print(f\"  Columns: {list(catalog_df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Note: Could not preview file: {e}\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"✗ Failed to download course catalog\")\n",
    "    print(\"  Please check:\")\n",
    "    print(\"    1. .env file contains valid SFTP_INBOUND_PASSWORD\")\n",
    "    print(\"    2. SFTP inbound server is accessible\")\n",
    "    print(\"    3. Remote path exists and contains CourseCatalog files\")\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Download Standalone Content\n",
    "print(\"Downloading Standalone Content...\")\n",
    "print(\"-\" * 80)\n",
    "standalone_content_path = download_most_recent_standalone_content()\n",
    "\n",
    "if standalone_content_path:\n",
    "    print()\n",
    "    print(f\"✓ Standalone content downloaded successfully\")\n",
    "    print(f\"  File: {standalone_content_path}\")\n",
    "    \n",
    "    # Optionally load and preview the file\n",
    "    try:\n",
    "        content_df = pd.read_csv(standalone_content_path)\n",
    "        print(f\"  Rows: {len(content_df)}\")\n",
    "        print(f\"  Columns: {list(content_df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Note: Could not preview file: {e}\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"✗ Failed to download standalone content\")\n",
    "    print(\"  Please check:\")\n",
    "    print(\"    1. .env file contains valid SFTP_INBOUND_PASSWORD\")\n",
    "    print(\"    2. SFTP inbound server is accessible\")\n",
    "    print(\"    3. Remote path exists and contains StandAloneContent files\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load employees using simulation_core\n",
    "print(f\"Loading employees from {EMPLOYEES_FILE}...\")\n",
    "employees_df, filtered_count = core.load_and_filter_employees(EMPLOYEES_FILE, print)\n",
    "print()\n",
    "\n",
    "# Use format_content_id from simulation_core\n",
    "format_content_id = core.format_content_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manager - Assign Training to Employees\n",
    "\n",
    "This section implements the manager functionality:\n",
    "1. Queries Databricks for employee state using BOTH content_assignments and content_completion tables\n",
    "2. Calculates open assignments (assignments - completions) for each employee\n",
    "3. Loads the standalone content file from preprocessing\n",
    "4. Filters for content where Daily_Dose_BA is TRUE\n",
    "5. Sorts by CreateDate (most recent first)\n",
    "6. Selects up to 3 Daily Dose contents to assign\n",
    "7. Checks for Daily Dose conflicts (employees to skip):\n",
    "   - **Conflict Check 1**: Queries content_completion table for employees who completed ANY Daily Dose this week\n",
    "   - **Conflict Check 2**: Checks open assignments for Daily Dose with due dates this week\n",
    "8. Skips employees who have Daily Dose conflicts\n",
    "9. Assigns the 3 Daily Dose contents to all eligible employees\n",
    "10. Assigns 1 random non-Daily Dose content to ALL employees (no skipping)\n",
    "11. Generates a NonCompletedAssignments CSV file with:\n",
    "    - Open assignments from Databricks (written FIRST)\n",
    "    - New manager assignments (written SECOND)\n",
    "\n",
    "**Note**: The manager will NOT assign Daily Dose to an employee if they already completed ANY Daily Dose in the current week (Sunday to Sunday).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "# Date/time helper functions - use simulation_core functions\n",
    "get_sunday_of_current_week = core.get_sunday_of_current_week\n",
    "get_next_future_sunday = core.get_next_future_sunday\n",
    "generate_request_id = core.generate_request_id\n",
    "generate_non_completed_assignments_filename = core.generate_non_completed_assignments_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for Databricks query that automatically uses config\n",
    "def get_open_assignments_from_databricks(employee_ids):\n",
    "    \"\"\"\n",
    "    Wrapper around core.get_open_assignments_from_databricks that uses the notebook's config.\n",
    "    \n",
    "    Args:\n",
    "        employee_ids: List of employee IDs to query\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with open assignments\n",
    "    \"\"\"\n",
    "    return core.get_open_assignments_from_databricks(config, employee_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manager - Query Databricks and select training content to assign to employees\n",
    "print(\"=\" * 80)\n",
    "print(\"MANAGER - Assigning Training to Employees\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Step 1: Get list of employee IDs from input file and query Databricks\n",
    "employee_ids_list = employees_df['employee_id'].tolist()\n",
    "print(f\"Querying Databricks for assignments for {len(employee_ids_list)} employees from input file...\")\n",
    "print(\"-\" * 80)\n",
    "open_assignments_df = get_open_assignments_from_databricks(employee_ids_list)\n",
    "print()\n",
    "\n",
    "# Convert Databricks assignments to the NonCompletedAssignments format using simulation_core\n",
    "databricks_assignments = core.convert_databricks_assignments_to_output_format(open_assignments_df)\n",
    "if databricks_assignments:\n",
    "    print(f\"Converted {len(databricks_assignments)} Databricks assignment(s) to output format\")\n",
    "    print()\n",
    "\n",
    "# Step 2: Load the standalone content file\n",
    "new_manager_assignments = []\n",
    "employee_assigned_daily_dose = {}  # Track Daily Dose assignments\n",
    "employee_assigned_random = {}  # Track random non-Daily Dose assignments\n",
    "\n",
    "if standalone_content_path and os.path.exists(standalone_content_path):\n",
    "    print(f\"Loading standalone content from: {standalone_content_path}\")\n",
    "    standalone_df = pd.read_csv(standalone_content_path)\n",
    "    print(f\"Loaded {len(standalone_df)} content items\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate dates for NEW assignments - functions now return UTC directly\n",
    "    created_date = datetime.now(PT).astimezone(UTC).isoformat()\n",
    "    start_date = get_sunday_of_current_week().isoformat()  # Already returns UTC\n",
    "    due_date = get_next_future_sunday().isoformat()  # Already returns UTC\n",
    "    \n",
    "    # PART A: DAILY DOSE ASSIGNMENTS\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PART A: DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Filter for content where Daily_Dose_BA is TRUE\n",
    "    print(\"Filtering for Daily Dose training (Daily_Dose_BA = TRUE)...\")\n",
    "    daily_dose_content = standalone_df[\n",
    "        (standalone_df['Daily_Dose_BA'] == 'TRUE') | \n",
    "        (standalone_df['Daily_Dose_BA'] == True)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Found {len(daily_dose_content)} Daily Dose content items\")\n",
    "    print()\n",
    "    \n",
    "    if len(daily_dose_content) > 0:\n",
    "        # Convert CreateDate to datetime for sorting\n",
    "        daily_dose_content['CreateDate_dt'] = pd.to_datetime(daily_dose_content['CreateDate'])\n",
    "        \n",
    "        # Sort by CreateDate (most recent first)\n",
    "        daily_dose_content = daily_dose_content.sort_values('CreateDate_dt', ascending=False)\n",
    "        \n",
    "        # Select up to 3 most recent contents\n",
    "        contents_to_assign = daily_dose_content.head(3)\n",
    "        \n",
    "        print(f\"Selected {len(contents_to_assign)} Daily Dose content(s) to assign:\")\n",
    "        for idx, content in contents_to_assign.iterrows():\n",
    "            content_id = content['ContentId']\n",
    "            content_name = content['ContentName']\n",
    "            create_date = content['CreateDate']\n",
    "            print(f\"  {format_content_id(int(content_id.replace(',', '')))} - {content_name} (Created: {create_date})\")\n",
    "        print()\n",
    "        \n",
    "        # Check which employees to skip for Daily Dose\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Checking for Daily Dose conflicts...\")\n",
    "        print()\n",
    "        \n",
    "        # Calculate current week boundaries (Monday to Monday)\n",
    "        now_pt = datetime.now(PT)\n",
    "        sunday_of_current_week = get_sunday_of_current_week()\n",
    "        next_sunday = get_next_future_sunday()\n",
    "        \n",
    "        # For completion date comparison, use date objects\n",
    "        week_start_date = sunday_of_current_week.date()\n",
    "        week_end_date = next_sunday.date()\n",
    "        \n",
    "        print(f\"Current week (for completion check): {week_start_date} to {week_end_date}\")\n",
    "        print()\n",
    "        \n",
    "        # Build set of Daily Dose content IDs for quick lookup\n",
    "        daily_dose_content_ids = set()\n",
    "        for _, dd_content in standalone_df[\n",
    "            (standalone_df['Daily_Dose_BA'] == 'TRUE') | \n",
    "            (standalone_df['Daily_Dose_BA'] == True)\n",
    "        ].iterrows():\n",
    "            content_id_str = dd_content['ContentId']\n",
    "            if isinstance(content_id_str, str):\n",
    "                content_id_numeric = int(content_id_str.replace(',', ''))\n",
    "            else:\n",
    "                content_id_numeric = int(content_id_str)\n",
    "            daily_dose_content_ids.add(content_id_numeric)\n",
    "        \n",
    "        # Check employees for Daily Dose conflicts\n",
    "        employees_to_skip_dd = {}  # Map employee_id -> reason for skipping Daily Dose\n",
    "        \n",
    "        # CONFLICT CHECK 1: Query content_completion for Daily Dose completions this week\n",
    "        if all([DATABRICKS_HOST, DATABRICKS_HTTP_PATH, DATABRICKS_TOKEN]):\n",
    "            try:\n",
    "                from databricks import sql\n",
    "                \n",
    "                # Connect to Databricks\n",
    "                connection = sql.connect(\n",
    "                    server_hostname=DATABRICKS_HOST,\n",
    "                    http_path=DATABRICKS_HTTP_PATH,\n",
    "                    access_token=DATABRICKS_TOKEN\n",
    "                )\n",
    "                \n",
    "                cursor = connection.cursor()\n",
    "                \n",
    "                # Table name\n",
    "                completion_table = f\"{DATABRICKS_CATALOG}.{DATABRICKS_SCHEMA}.content_completion\"\n",
    "                \n",
    "                print(f\"Querying {completion_table} for Daily Dose completions this week...\")\n",
    "                \n",
    "                # Build IN clause for employee IDs and Daily Dose content IDs\n",
    "                employee_ids_str = \", \".join([str(emp_id) for emp_id in employee_ids_list])\n",
    "                dd_content_ids_str = \", \".join([str(cid) for cid in daily_dose_content_ids])\n",
    "                \n",
    "                # Query: Find employees who completed ANY Daily Dose content this week\n",
    "                query = f\"\"\"\n",
    "                SELECT \n",
    "                    ba_id,\n",
    "                    content_id,\n",
    "                    completion_date\n",
    "                FROM {completion_table}\n",
    "                WHERE ba_id IN ({employee_ids_str})\n",
    "                    AND content_id IN ({dd_content_ids_str})\n",
    "                    AND completion_date >= '{week_start_date}'\n",
    "                    AND completion_date <= '{week_end_date}'\n",
    "                ORDER BY ba_id, completion_date DESC\n",
    "                \"\"\"\n",
    "                \n",
    "                cursor.execute(query)\n",
    "                \n",
    "                # Fetch results\n",
    "                completion_rows = cursor.fetchall()\n",
    "                \n",
    "                # Close connection\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "                \n",
    "                if completion_rows:\n",
    "                    print(f\"Found {len(completion_rows)} Daily Dose completion(s) this week:\")\n",
    "                    for row in completion_rows:\n",
    "                        emp_id = row[0]\n",
    "                        content_id = row[1]\n",
    "                        comp_date = row[2]\n",
    "                        \n",
    "                        # Mark this employee to skip\n",
    "                        if emp_id not in employees_to_skip_dd:\n",
    "                            employees_to_skip_dd[emp_id] = {\n",
    "                                'reason': 'completed_this_week',\n",
    "                                'content_id': content_id,\n",
    "                                'completion_date': comp_date\n",
    "                            }\n",
    "                        \n",
    "                        print(f\"  Employee {emp_id}: Completed Daily Dose {content_id} on {comp_date}\")\n",
    "                    print()\n",
    "                else:\n",
    "                    print(\"✓ No Daily Dose completions found this week\")\n",
    "                    print()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Could not query content_completion: {e}\")\n",
    "                print(\"  Continuing without completion check...\")\n",
    "                print()\n",
    "        \n",
    "        # CONFLICT CHECK 2: Check open assignments for Daily Dose with due dates this week\n",
    "        today_date = now_pt.date()\n",
    "        next_sunday_date = next_sunday.date()\n",
    "        \n",
    "        if not open_assignments_df.empty:\n",
    "            for _, assignment in open_assignments_df.iterrows():\n",
    "                employee_id = int(assignment['ba_id'])\n",
    "                content_id = int(assignment['content_id'])\n",
    "                assignment_due_date = assignment['assignment_due_date']\n",
    "                \n",
    "                # Check if this is a Daily Dose assignment\n",
    "                if content_id in daily_dose_content_ids:\n",
    "                    # Convert due date to date for comparison\n",
    "                    if hasattr(assignment_due_date, 'date'):\n",
    "                        due_date_check = assignment_due_date.date()\n",
    "                    else:\n",
    "                        from dateutil import parser\n",
    "                        due_date_check = parser.parse(str(assignment_due_date)).date()\n",
    "                    \n",
    "                    # Check if due date is today or next future Monday\n",
    "                    if due_date_check == today_date or due_date_check == next_sunday_date:\n",
    "                        if employee_id not in employees_to_skip_dd:\n",
    "                            employees_to_skip_dd[employee_id] = {\n",
    "                                'reason': 'open_assignment_this_week',\n",
    "                                'content_id': content_id,\n",
    "                                'due_date': due_date_check\n",
    "                            }\n",
    "        \n",
    "        # Log employees who will be skipped for Daily Dose\n",
    "        if employees_to_skip_dd:\n",
    "            print(f\"❌ SKIPPING {len(employees_to_skip_dd)} employee(s) - Daily Dose conflict:\")\n",
    "            print()\n",
    "            for emp_id in sorted(employees_to_skip_dd.keys()):\n",
    "                skip_info = employees_to_skip_dd[emp_id]\n",
    "                print(f\"  Employee {emp_id}:\")\n",
    "                \n",
    "                if skip_info['reason'] == 'completed_this_week':\n",
    "                    print(f\"    Reason: Already completed Daily Dose this week\")\n",
    "                    print(f\"    Completed Content: {skip_info['content_id']}\")\n",
    "                    print(f\"    Completion Date: {skip_info['completion_date']}\")\n",
    "                else:\n",
    "                    print(f\"    Reason: Has open Daily Dose assignment for this week\")\n",
    "                    print(f\"    Assigned Content: {skip_info['content_id']}\")\n",
    "                    print(f\"    Due Date: {skip_info['due_date']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"✓ No Daily Dose conflicts found\")\n",
    "            print()\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print()\n",
    "        \n",
    "        # Create Daily Dose assignments for eligible employees\n",
    "        eligible_employees_dd = [emp for _, emp in employees_df.iterrows() \n",
    "                                 if emp['employee_id'] not in employees_to_skip_dd]\n",
    "        \n",
    "        if len(eligible_employees_dd) > 0:\n",
    "            print(f\"✓ ASSIGNING DAILY DOSE TO {len(eligible_employees_dd)} ELIGIBLE EMPLOYEE(S)\")\n",
    "            print()\n",
    "            \n",
    "            for employee in eligible_employees_dd:\n",
    "                employee_id = employee['employee_id']\n",
    "                \n",
    "                print(f\"Employee {employee_id}:\")\n",
    "                \n",
    "                # Assign each selected Daily Dose content to this employee\n",
    "                for _, content in contents_to_assign.iterrows():\n",
    "                    content_id = content['ContentId']\n",
    "                    content_name = content['ContentName']\n",
    "                    \n",
    "                    print(f\"  ✓ Daily Dose: {content_id} - {content_name}\")\n",
    "                    \n",
    "                    # Store what was assigned\n",
    "                    if employee_id not in employee_assigned_daily_dose:\n",
    "                        employee_assigned_daily_dose[employee_id] = []\n",
    "                    employee_assigned_daily_dose[employee_id].append({\n",
    "                        'content_id': content_id,\n",
    "                        'content_name': content_name\n",
    "                    })\n",
    "                    \n",
    "                    assignment = {\n",
    "                        \"UserID\": employee_id,\n",
    "                        \"CreateDate_text\": created_date,\n",
    "                        \"RequestId\": generate_request_id(),\n",
    "                        \"TrainingElementId\": content_id,\n",
    "                        \"Start_Date_text\": start_date,\n",
    "                        \"DueDate_text\": due_date,\n",
    "                        \"ContentType\": \"Media\"\n",
    "                    }\n",
    "                    \n",
    "                    new_manager_assignments.append(assignment)\n",
    "                \n",
    "                print()\n",
    "            \n",
    "            dd_count = len([a for a in new_manager_assignments])\n",
    "            print(f\"Created {dd_count} Daily Dose assignments\")\n",
    "            print()\n",
    "        else:\n",
    "            print(\"⚠ NO ELIGIBLE EMPLOYEES for Daily Dose\")\n",
    "            print(\"All employees have Daily Dose conflicts (completed or assigned this week).\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No Daily Dose content found.\")\n",
    "        print()\n",
    "    \n",
    "    # PART B: RANDOM NON-DAILY DOSE ASSIGNMENTS\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PART B: RANDOM NON-DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Filter for content where Daily_Dose_BA is NOT TRUE\n",
    "    print(\"Filtering for NON-Daily Dose training (Daily_Dose_BA != TRUE)...\")\n",
    "    non_daily_dose_content = standalone_df[\n",
    "        ~((standalone_df['Daily_Dose_BA'] == 'TRUE') | \n",
    "          (standalone_df['Daily_Dose_BA'] == True))\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Found {len(non_daily_dose_content)} non-Daily Dose content items\")\n",
    "    print()\n",
    "    \n",
    "    if len(non_daily_dose_content) > 0:\n",
    "        # Collect assignment data for table display\n",
    "        random_assignment_rows = []\n",
    "        \n",
    "        for _, employee in employees_df.iterrows():\n",
    "            employee_id = employee['employee_id']\n",
    "            \n",
    "            # Randomly select one content from non-Daily Dose content\n",
    "            selected_content = non_daily_dose_content.sample(n=1).iloc[0]\n",
    "            content_id = selected_content['ContentId']\n",
    "            content_name = selected_content['ContentName']\n",
    "            \n",
    "            # Store for table display\n",
    "            random_assignment_rows.append((employee_id, content_id, content_name))\n",
    "            \n",
    "            # Store what was assigned\n",
    "            employee_assigned_random[employee_id] = {\n",
    "                'content_id': content_id,\n",
    "                'content_name': content_name\n",
    "            }\n",
    "            \n",
    "            assignment = {\n",
    "                \"UserID\": employee_id,\n",
    "                \"CreateDate_text\": created_date,\n",
    "                \"RequestId\": generate_request_id(),\n",
    "                \"TrainingElementId\": content_id,\n",
    "                \"Start_Date_text\": start_date,\n",
    "                \"DueDate_text\": due_date,\n",
    "                \"ContentType\": \"Media\"\n",
    "            }\n",
    "            \n",
    "            new_manager_assignments.append(assignment)\n",
    "        \n",
    "        # Print header\n",
    "        print(f\"Random non-Daily Dose training assigned to {len(employees_df)} employee(s):\")\n",
    "        print()\n",
    "        print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Content Name'}\")\n",
    "        print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "        \n",
    "        # Print each assignment\n",
    "        for emp_id, content_id, content_name in sorted(random_assignment_rows):\n",
    "            print(f\"{emp_id:<15} | {content_id:<15} | {content_name}\")\n",
    "        \n",
    "        print()\n",
    "        random_count = len(employee_assigned_random)\n",
    "        print(f\"Created {random_count} random non-Daily Dose assignments\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No non-Daily Dose content found.\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Standalone content file not found.\")\n",
    "    print(\"Please run the preprocessing section first.\")\n",
    "\n",
    "# Step 3: Combine Databricks assignments with new manager assignments\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINING ASSIGNMENTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"  Assignments from Databricks Table: {len(databricks_assignments)}\")\n",
    "print(f\"  New Daily Dose assignments: {len(employee_assigned_daily_dose) * len(contents_to_assign) if employee_assigned_daily_dose else 0}\")\n",
    "print(f\"  New random Non-Daily Dose assignments: {len(employee_assigned_random)}\")\n",
    "print(f\"  Total new assignments: {len(new_manager_assignments)}\")\n",
    "\n",
    "# Databricks assignments go FIRST (as per manager.md)\n",
    "all_assignments = databricks_assignments + new_manager_assignments\n",
    "print(f\"  Total assignments for output: {len(all_assignments)}\")\n",
    "print()\n",
    "\n",
    "# Step 4: Generate output file\n",
    "if all_assignments:\n",
    "    assignments_filename = generate_non_completed_assignments_filename()\n",
    "    assignments_path = f\"{OUTPUT_DIR}/{assignments_filename}\"\n",
    "    \n",
    "    # Create DataFrame\n",
    "    assignments_df = pd.DataFrame(all_assignments)\n",
    "    \n",
    "    # Write to CSV with proper quoting\n",
    "    assignments_df.to_csv(assignments_path, index=False, quoting=1)  # quoting=1 means QUOTE_ALL\n",
    "    \n",
    "    print(f\"Generated NonCompletedAssignments file: {assignments_filename}\")\n",
    "    print(f\"  Databricks Table assignments: {len(databricks_assignments)}\")\n",
    "    print(f\"  New manager assignments: {len(new_manager_assignments)}\")\n",
    "    print(f\"  Total assignments in file: {len(all_assignments)}\")\n",
    "    print()\n",
    "    \n",
    "    # Print summary for NEW assignments\n",
    "    if new_manager_assignments:\n",
    "        print(\"New Assignment Summary:\")\n",
    "        if employee_assigned_daily_dose:\n",
    "            print(f\"  Daily Dose assignments: {len(employee_assigned_daily_dose)} employee(s) × {len(contents_to_assign)} contents = {len(employee_assigned_daily_dose) * len(contents_to_assign)}\")\n",
    "            if employees_to_skip_dd:\n",
    "                print(f\"  Skipped (Daily Dose conflicts): {len(employees_to_skip_dd)} employee(s)\")\n",
    "        if employee_assigned_random:\n",
    "            print(f\"  Random assignments: {len(employee_assigned_random)} employee(s) × 1 content = {len(employee_assigned_random)}\")\n",
    "        print(f\"  Total new assignments: {len(new_manager_assignments)}\")\n",
    "        print(f\"  CreateDate: {created_date}\")\n",
    "        print(f\"  Start Date: {start_date}\")\n",
    "        print(f\"  Due Date: {due_date}\")\n",
    "else:\n",
    "    print(\"No assignments created (neither from Databricks nor new manager assignments).\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Training Simulation\n",
    "\n",
    "This section simulates employees completing training based on manager assignments and AI recommendations:\n",
    "\n",
    "## Workflow:\n",
    "1. **Get Recommendations** (next cell): Calls ML Training Recommender API for each employee\n",
    "2. **Get Manager Assignments**: Loads assignments from NonCompletedAssignments file created by manager\n",
    "3. **Combine Training**: Merges manager assignments with AI recommendations\n",
    "4. **Check Recent Completions**: Queries content_completion table for training completed in last 13 days\n",
    "   - **ONLY applies to AI recommendations** - manager assignments are ALWAYS included\n",
    "   - Employees skip AI-recommended training they completed recently (today + prior 12 days)\n",
    "   - Filtered AI training is removed from available training list\n",
    "   - Logs which AI training was skipped and why\n",
    "5. **Helper Functions** (following cell): Generates training timestamps\n",
    "6. **Process Employee**: Determines completions based on employee type:\n",
    "   - Type A: Completes all training (manager + filtered AI)\n",
    "   - Type B: Completes one training (from combined list of manager + filtered AI)\n",
    "   - Type F: Completes no training\n",
    "7. **Filename Generator**: Creates unique output filename with timestamp\n",
    "8. **Main Loop**: Processes all employees and collects completion records\n",
    "9. **Generate Output**: Writes ContentUserCompletion CSV file\n",
    "10. **Update NonCompletedAssignments**: Removes completed training from NonCompletedAssignments file\n",
    "11. **Print Summary**: Displays completion summary with source (manager or AI) for each employee\n",
    "\n",
    "**Important Notes**: \n",
    "- Employees will skip AI-recommended training they already completed in the last 13 days (current day + prior 12 days)\n",
    "- Manager-assigned training is NEVER skipped - employees always see and complete manager assignments regardless of recent completion history\n",
    "- This prevents duplicate AI recommendations while ensuring manager assignments are always honored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for ML API calls that automatically uses config\n",
    "def get_training_recommendations(employee_id):\n",
    "    \"\"\"\n",
    "    Wrapper around core.get_training_recommendations that uses the notebook's config.\n",
    "    \n",
    "    Args:\n",
    "        employee_id: The employee's ID\n",
    "    \n",
    "    Returns:\n",
    "        List of training recommendations\n",
    "    \"\"\"\n",
    "    return core.get_training_recommendations(config, employee_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use simulation_core function for generating training times\n",
    "generate_training_times = core.generate_training_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use simulation_core functions for employee processing\n",
    "generate_output_filename = core.generate_output_filename\n",
    "\n",
    "# Wrapper function for process_employee to adapt parameter order for notebook usage\n",
    "def process_employee(employee_id: int, employee_type: str, manager_assignments_path: str, standalone_df, ai_recommendations = None):\n",
    "    \"\"\"\n",
    "    Wrapper around simulation_core.process_employee that adapts the signature for notebook usage.\n",
    "    \n",
    "    Args:\n",
    "        employee_id: The employee's ID\n",
    "        employee_type: The employee's type (a, b, or f)\n",
    "        manager_assignments_path: Path to the NonCompletedAssignments CSV file\n",
    "        standalone_df: DataFrame containing standalone content for lookups\n",
    "        ai_recommendations: Optional pre-fetched AI recommendations\n",
    "    \n",
    "    Returns:\n",
    "        List of completed training records\n",
    "    \"\"\"\n",
    "    # Get manager assignments using simulation_core helper\n",
    "    manager_assignments = core.get_manager_assignments_for_employee(\n",
    "        employee_id, manager_assignments_path, standalone_df)\n",
    "    \n",
    "    # Get AI recommendations if not provided\n",
    "    if ai_recommendations is None:\n",
    "        ai_recommendations = core.get_training_recommendations(config, employee_id)\n",
    "    \n",
    "    # Call simulation_core.process_employee\n",
    "    return core.process_employee(\n",
    "        config,\n",
    "        employee_id,\n",
    "        employee_type,\n",
    "        manager_assignments,\n",
    "        ai_recommendations,\n",
    "        standalone_df\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution - Process employees and simulate training completions\n",
    "print(\"=\" * 80)\n",
    "print(\"EMPLOYEE TRAINING SIMULATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check if manager assignments were created\n",
    "if 'assignments_path' not in locals() or not os.path.exists(assignments_path):\n",
    "    print(\"WARNING: Manager assignments file not found. Employees will only complete AI recommendations.\")\n",
    "    print()\n",
    "    assignments_path = \"\"\n",
    "\n",
    "# Process each employee\n",
    "all_completions = []\n",
    "employee_summaries = []\n",
    "employee_ml_recommendations = []  # Store ML recommendations for summary\n",
    "\n",
    "for _, employee in employees_df.iterrows():\n",
    "    employee_id = employee['employee_id']\n",
    "    employee_type = employee['employee_edu_type']\n",
    "    \n",
    "    print(f\"Processing Employee {employee_id} (Type {employee_type.upper()})...\")\n",
    "    \n",
    "    # Get AI recommendations\n",
    "    ai_recommendations = get_training_recommendations(employee_id)\n",
    "    \n",
    "    # Store ML recommendations for this employee\n",
    "    if ai_recommendations:\n",
    "        ml_recs = []\n",
    "        for rec in ai_recommendations:\n",
    "            ml_recs.append({\n",
    "                \"content_id\": rec.get(\"recommended_content_id\"),\n",
    "                \"content_name\": rec.get(\"recommended_content\", \"Unknown\")\n",
    "            })\n",
    "        employee_ml_recommendations.append((employee_id, ml_recs))\n",
    "    \n",
    "    # Process employee with pre-fetched AI recommendations\n",
    "    completions = process_employee(employee_id, employee_type, assignments_path, standalone_df, ai_recommendations)\n",
    "    \n",
    "    if completions:\n",
    "        all_completions.extend(completions)\n",
    "        # Store ContentId, CourseName, and Source for summary\n",
    "        course_details = [(c['ContentId'], c['CourseName'], c['Source']) for c in completions]\n",
    "        employee_summaries.append((employee_id, course_details))\n",
    "        print(f\"  Completed {len(completions)} training(s)\")\n",
    "    else:\n",
    "        print(f\"  No training completed\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output file\n",
    "if all_completions:\n",
    "    # Use simulation_core function to write the output file\n",
    "    output_path = core.write_content_user_completion_file(all_completions, OUTPUT_DIR)\n",
    "    \n",
    "    output_filename = os.path.basename(output_path)\n",
    "    print(f\"Generated output file: {output_filename}\")\n",
    "    print(f\"Total completions: {len(all_completions)}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"No training completions to write.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update NonCompletedAssignments file to remove completed training\n",
    "if all_completions and 'assignments_path' in locals() and os.path.exists(assignments_path):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"UPDATING NON-COMPLETED ASSIGNMENTS FILE\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Use simulation_core function to update the assignments file\n",
    "    initial_count, removed_count = core.update_non_completed_assignments_file(\n",
    "        assignments_path, all_completions)\n",
    "    \n",
    "    remaining_count = initial_count - removed_count\n",
    "    \n",
    "    print(f\"Updated NonCompletedAssignments file:\")\n",
    "    print(f\"  File: {assignments_path}\")\n",
    "    print(f\"  Initial assignments: {initial_count}\")\n",
    "    print(f\"  Completed assignments (removed): {removed_count}\")\n",
    "    print(f\"  Remaining assignments: {remaining_count}\")\n",
    "    \n",
    "    if remaining_count == 0:\n",
    "        print()\n",
    "        print(\"✓ All assignments completed!\")\n",
    "        print(\"  File contains headers only (no remaining assignments)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "elif all_completions:\n",
    "    print(\"⚠ NonCompletedAssignments file not found - skipping update\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"⚠ No completions to process - skipping NonCompletedAssignments update\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"-\" * 80)\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MANAGER-ASSIGNMENTS GIVEN NEW\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# PART A: Daily Dose Assignments\n",
    "if 'employee_assigned_daily_dose' in locals() and len(employee_assigned_daily_dose) > 0:\n",
    "    print(\"PART A: DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    print(f\"Daily Dose training assigned to {len(employee_assigned_daily_dose)} employee(s):\")\n",
    "    print()\n",
    "    \n",
    "    # Get the contents from the first employee (they all have the same Daily Dose)\n",
    "    first_employee_contents = list(employee_assigned_daily_dose.values())[0]\n",
    "    \n",
    "    print(\"Daily Dose Contents:\")\n",
    "    for content_info in first_employee_contents:\n",
    "        content_id = content_info['content_id']\n",
    "        content_name = content_info['content_name']\n",
    "        print(f\"  {content_id} - {content_name}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Print list of all employees who received these assignments\n",
    "    employee_ids_str = \", \".join([str(emp_id) for emp_id in sorted(employee_assigned_daily_dose.keys())])\n",
    "    print(f\"Employees: {employee_ids_str}\")\n",
    "    print()\n",
    "    \n",
    "    if 'employees_to_skip_dd' in locals() and employees_to_skip_dd:\n",
    "        skipped_ids_str = \", \".join([str(emp_id) for emp_id in sorted(employees_to_skip_dd.keys())])\n",
    "        print(f\"Skipped (already have Daily Dose): {skipped_ids_str}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"PART A: DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    print(\"No new Daily Dose assignments were created in this run.\")\n",
    "    if 'employees_to_skip_dd' in locals() and employees_to_skip_dd:\n",
    "        print(f\"All {len(employees_to_skip_dd)} employee(s) already have Daily Dose for current week.\")\n",
    "    print()\n",
    "\n",
    "# PART B: Random Non-Daily Dose Assignments\n",
    "if 'employee_assigned_random' in locals() and len(employee_assigned_random) > 0:\n",
    "    print(\"PART B: RANDOM NON-DAILY DOSE ASSIGNMENTS\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    print(f\"Random non-Daily Dose training assigned to {len(employee_assigned_random)} employee(s):\")\n",
    "    print()\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Course Name'}\")\n",
    "    print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "    \n",
    "    # Print each employee's random assignment\n",
    "    for emp_id in sorted(employee_assigned_random.keys()):\n",
    "        content_info = employee_assigned_random[emp_id]\n",
    "        content_id = content_info['content_id']\n",
    "        content_name = content_info['content_name']\n",
    "        print(f\"{emp_id:<15} | {content_id:<15} | {content_name}\")\n",
    "    \n",
    "    print()\n",
    "else:\n",
    "    print(\"PART B: NON-DAILY DOSE ASSIGNMENTS RAMDOMLY CHOSEN\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    print(\"No random non-Daily Dose assignments were created in this run.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RECOMMENDATIONS GIVEN BY ML API\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Display all ML recommendations given to employees in pipe-separated format\n",
    "if employee_ml_recommendations:\n",
    "    # Collect all recommendation rows\n",
    "    recommendation_rows = []\n",
    "    \n",
    "    for employee_id, ml_recs in employee_ml_recommendations:\n",
    "        for rec in ml_recs:\n",
    "            content_id = rec[\"content_id\"]\n",
    "            content_name = rec[\"content_name\"]\n",
    "            recommendation_rows.append((employee_id, content_id, content_name))\n",
    "    \n",
    "    # Sort by employee ID, then content ID\n",
    "    recommendation_rows.sort(key=lambda x: (x[0], str(x[1])))\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Content Name'}\")\n",
    "    print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "    \n",
    "    # Print each recommendation as a separate row\n",
    "    for employee_id, content_id, content_name in recommendation_rows:\n",
    "        print(f\"{employee_id:<15} | {content_id:<15} | {content_name}\")\n",
    "    \n",
    "    print()\n",
    "else:\n",
    "    print(\"No ML recommendations were given to any employee.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EMPLOYEE TRAINING COMPLETIONS OF MANAGER-ASSIGNED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Track if any manager assignments were completed\n",
    "manager_completions_found = False\n",
    "\n",
    "# Collect all manager completions for table display\n",
    "manager_completion_rows = []\n",
    "\n",
    "for employee_id, course_details in employee_summaries:\n",
    "    # Filter for manager-assigned training only\n",
    "    manager_courses = [(content_id, course_name) for content_id, course_name, source in course_details if source == \"manager\"]\n",
    "    \n",
    "    if manager_courses:\n",
    "        manager_completions_found = True\n",
    "        for content_id, course_name in manager_courses:\n",
    "            manager_completion_rows.append((employee_id, content_id, course_name))\n",
    "\n",
    "if manager_completions_found:\n",
    "    # Sort by employee ID, then content ID\n",
    "    manager_completion_rows.sort(key=lambda x: (x[0], str(x[1])))\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Course Name'}\")\n",
    "    print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "    \n",
    "    # Print each completion on a separate row\n",
    "    for employee_id, content_id, course_name in manager_completion_rows:\n",
    "        print(f\"{employee_id:<15} | {content_id:<15} | {course_name}\")\n",
    "else:\n",
    "    print(\"No manager-assigned training was completed by any employee.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\" EMPLOYEE TRAINING COMPLETIONS OF ML-RECOMMENDED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Track if any ML recommendations were completed\n",
    "ml_completions_found = False\n",
    "\n",
    "# Collect all ML completions for table display\n",
    "ml_completion_rows = []\n",
    "\n",
    "for employee_id, course_details in employee_summaries:\n",
    "    # Filter for ML-recommended training only\n",
    "    ml_courses = [(content_id, course_name) for content_id, course_name, source in course_details if source == \"ai\"]\n",
    "    \n",
    "    if ml_courses:\n",
    "        ml_completions_found = True\n",
    "        for content_id, course_name in ml_courses:\n",
    "            ml_completion_rows.append((employee_id, content_id, course_name))\n",
    "\n",
    "if ml_completions_found:\n",
    "    # Sort by employee ID, then content ID\n",
    "    ml_completion_rows.sort(key=lambda x: (x[0], str(x[1])))\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Employee ID':<15} | {'Content ID':<15} | {'Course Name'}\")\n",
    "    print(f\"{'-' * 15} | {'-' * 15} | {'-' * 50}\")\n",
    "    \n",
    "    # Print each completion on a separate row\n",
    "    for employee_id, content_id, course_name in ml_completion_rows:\n",
    "        print(f\"{employee_id:<15} | {content_id:<15} | {course_name}\")\n",
    "else:\n",
    "    print(\"No ML-recommended training was completed by any employee.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"execution complete\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing - Publish generated files to SFTP outbound server\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"POSTPROCESSING - Publish Files to SFTP Outbound Server\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check if publishing is enabled\n",
    "if not SFTP_PUBLISH_ENABLED:\n",
    "    print(\"⊘ SFTP publishing is DISABLED\")\n",
    "    print(f\"  To enable publishing, set SFTP_PUBLISH_ENABLED=true in .env file\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"✓ SFTP publishing is ENABLED\")\n",
    "    print()\n",
    "    \n",
    "    # Collect all files to publish\n",
    "    files_to_publish = []\n",
    "    \n",
    "    # Add generated output files\n",
    "    if 'output_path' in locals() and os.path.exists(output_path):\n",
    "        files_to_publish.append(output_path)\n",
    "    \n",
    "    if 'assignments_path' in locals() and os.path.exists(assignments_path):\n",
    "        files_to_publish.append(assignments_path)\n",
    "    \n",
    "    if 'user_completion_path' in locals() and os.path.exists(user_completion_path):\n",
    "        files_to_publish.append(user_completion_path)\n",
    "    \n",
    "    # Add downloaded files (preprocessing files)\n",
    "    if 'course_catalog_path' in locals() and os.path.exists(course_catalog_path):\n",
    "        files_to_publish.append(course_catalog_path)\n",
    "    \n",
    "    if 'standalone_content_path' in locals() and os.path.exists(standalone_content_path):\n",
    "        files_to_publish.append(standalone_content_path)\n",
    "    \n",
    "    if files_to_publish:\n",
    "        print(f\"Files to publish ({len(files_to_publish)}):\")\n",
    "        print()\n",
    "        \n",
    "        # Categorize files for better display\n",
    "        generated_files = []\n",
    "        downloaded_files = []\n",
    "        \n",
    "        for file_path in files_to_publish:\n",
    "            filename = os.path.basename(file_path)\n",
    "            if 'course_catalog_path' in locals() and file_path == course_catalog_path:\n",
    "                downloaded_files.append(filename)\n",
    "            elif 'standalone_content_path' in locals() and file_path == standalone_content_path:\n",
    "                downloaded_files.append(filename)\n",
    "            else:\n",
    "                generated_files.append(filename)\n",
    "        \n",
    "        if generated_files:\n",
    "            print(\"Generated files:\")\n",
    "            for filename in generated_files:\n",
    "                print(f\"  - {filename}\")\n",
    "        \n",
    "        if downloaded_files:\n",
    "            print()\n",
    "            print(\"Downloaded files (from preprocessing):\")\n",
    "            for filename in downloaded_files:\n",
    "                print(f\"  - {filename}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Publish files\n",
    "        print(\"Publishing files...\")\n",
    "        print(\"-\" * 80)\n",
    "        success = publish_files_to_sftp_outbound(files_to_publish)\n",
    "        print(\"-\" * 80)\n",
    "        print()\n",
    "        \n",
    "        if success:\n",
    "            print(\"✓ All files published successfully\")\n",
    "        else:\n",
    "            print(\"⚠ Some files failed to publish\")\n",
    "    else:\n",
    "        print(\"⚠ No files found to publish\")\n",
    "        print(\"  Generated files may not exist. Please run the notebook cells in order.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
